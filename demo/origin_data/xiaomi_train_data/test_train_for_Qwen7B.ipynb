{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67619e5-329a-47a3-ab3e-f5bbbe38fdca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T11:45:19.637705Z",
     "iopub.status.busy": "2024-02-14T11:45:19.637428Z",
     "iopub.status.idle": "2024-02-14T11:45:32.661620Z",
     "shell.execute_reply": "2024-02-14T11:45:32.661086Z",
     "shell.execute_reply.started": "2024-02-14T11:45:19.637678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 7097, done.\u001b[K\n",
      "remote: Counting objects: 100% (1116/1116), done.\u001b[K\n",
      "remote: Compressing objects: 100% (414/414), done.\u001b[K\n",
      "remote: Total 7097 (delta 781), reused 982 (delta 699), pack-reused 5981\u001b[K\n",
      "接收对象中: 100% (7097/7097), 204.88 MiB | 24.09 MiB/s, 完成.\n",
      "处理 delta 中: 100% (5132/5132), 完成.\n",
      "正在更新文件: 100% (140/140), 完成.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tking007/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cebff7a-7f9d-4d28-bfa7-9f0d86ee8ca8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-14T11:45:32.662751Z",
     "iopub.status.busy": "2024-02-14T11:45:32.662533Z",
     "iopub.status.idle": "2024-02-14T11:45:34.181997Z",
     "shell.execute_reply": "2024-02-14T11:45:34.181389Z",
     "shell.execute_reply.started": "2024-02-14T11:45:32.662736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正克隆到 'train_data_for_Qwen'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 25 (delta 9), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "接收对象中: 100% (25/25), 9.29 KiB | 9.29 MiB/s, 完成.\n",
      "处理 delta 中: 100% (9/9), 完成.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://www.modelscope.cn/datasets/j869903116/train_data_for_Qwen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284a6a24-0918-480b-b430-b7ec1ccc7dc1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-14T11:46:05.966010Z",
     "iopub.status.busy": "2024-02-14T11:46:05.965693Z",
     "iopub.status.idle": "2024-02-14T11:46:05.969743Z",
     "shell.execute_reply": "2024-02-14T11:46:05.969255Z",
     "shell.execute_reply.started": "2024-02-14T11:46:05.965989Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/workspace/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6562ba99-4b6e-4b0f-8643-1dcbc000ec3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-14T11:46:08.427366Z",
     "iopub.status.busy": "2024-02-14T11:46:08.427064Z",
     "iopub.status.idle": "2024-02-14T11:46:19.955048Z",
     "shell.execute_reply": "2024-02-14T11:46:19.954397Z",
     "shell.execute_reply.started": "2024-02-14T11:46:08.427348Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.2+cu121)\n",
      "Collecting transformers>=4.37.2 (from -r requirements.txt (line 2))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/85/f6/c5065913119c41ecad148c34e3a861f719e16b89a522287213698da911fc/transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.14.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.16.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.25.0)\n",
      "Requirement already satisfied: peft>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: trl>=0.7.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.7.7)\n",
      "Collecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/bd/ea/ca6506e4da9b5338da3bfdd6115dc1c90ffd58c1ec50ca2792b84a7b4bdb/gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.42.1)\n",
      "Collecting rouge-chinese (from -r requirements.txt (line 13))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.8.1)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.25.0)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.10.13)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.108.0)\n",
      "Requirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (1.8.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 4)) (5.9.7)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.7.6->-r requirements.txt (line 6)) (0.6.3)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/1d/70/07914754979f5dd80bda947a0ffd181c08bfcb137b01c3c0cef45254d271/ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.6.1 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/7d/04/e1654ee28fb2686514ca8ae31af5e489403964d48764788f9a168e069c0f/gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-resources<7.0,>=1.3 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/93/e8/facde510585869b5ec694e8e0363ffe4eba067cb357a8398a55f6a1f8023/importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (10.2.0)\n",
      "Collecting pydub (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/3d/47/444768600d9e0ebc82f8e347775d24aef8f6348cf00e9fa0e81910814e6d/python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/58/0a/7570e15661a0a546c3a1152d95fe8c05480459bab36247f0acbf41f01a41/websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge-chinese->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 14)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 14)) (1.3.2)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 15)) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 17)) (0.32.0.post1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 18)) (4.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 19)) (2.8.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.20.0)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/b7/8a/d82202c9f89eab30f9fc05380daae87d617e2ad11571ab23d7c13a29bb54/toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 18)) (1.2.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (1.6.5)\n",
      "Collecting httpcore==1.* (from httpx->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.16.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.7.6->-r requirements.txt (line 6)) (0.1.2)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5598 sha256=656d1b6ff25b268aa176029c6c56389563fcd1060cb41ccebaf16e61d709f87b\n",
      "  Stored in directory: /root/.cache/pip/wheels/d5/ba/8f/dde85c39f765c48a489063a576d1ec23a0097b155e2fc243f0\n",
      "Successfully built ffmpy\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pydub, ffmpy, websockets, toolz, semantic-version, rouge-chinese, python-multipart, importlib-resources, httpcore, aiofiles, httpx, gradio-client, transformers, altair, gradio\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 12.0\n",
      "    Uninstalling websockets-12.0:\n",
      "      Successfully uninstalled websockets-12.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ms-swift 1.5.1 requires transformers<4.37,>=4.33, but you have transformers 4.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiofiles-23.2.1 altair-5.2.0 ffmpy-0.3.2 gradio-3.50.2 gradio-client-0.6.1 httpcore-1.0.2 httpx-0.26.0 importlib-resources-6.1.1 pydub-0.25.1 python-multipart-0.0.9 rouge-chinese-1.0.3 semantic-version-2.10.0 toolz-0.12.1 transformers-4.37.2 websockets-11.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915104c-6b7a-49d9-b47f-268bdeaef1c0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-14T11:46:42.875162Z",
     "iopub.status.busy": "2024-02-14T11:46:42.874835Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-14 19:46:49.919666: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-14 19:46:49.991739: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-14 19:46:50.325194: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-14 19:46:50.325228: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-14 19:46:50.327471: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-14 19:46:50.517364: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-14 19:46:50.518766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-14 19:46:51.677978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-02-14 19:46:54,388] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "Could not create share link. Missing file: /opt/conda/lib/python3.10/site-packages/gradio/frpc_linux_amd64_v0.2. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.2\n",
      "3. Move the file to this location: /opt/conda/lib/python3.10/site-packages/gradio\n",
      "02/14/2024 19:47:44 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:47:44,048 >> PyTorch: setting up devices\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "02/14/2024 19:47:44 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "02/14/2024 19:47:44 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=saves/Qwen-7B/lora/train_2024-02-14-19-47-02/runs/Feb14_19-47-44_dsw-288938-6d9f6bc866-5wj5n,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=saves/Qwen-7B/lora/train_2024-02-14-19-47-02,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=saves/Qwen-7B/lora/train_2024-02-14-19-47-02,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-02-14 19:47:44,098 - modelscope - INFO - PyTorch version 2.1.2+cu121 Found.\n",
      "2024-02-14 19:47:44,099 - modelscope - INFO - TensorFlow version 2.14.0 Found.\n",
      "2024-02-14 19:47:44,099 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-02-14 19:47:44,099 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-02-14 19:47:44,366 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 23d6ca2be1734e5d30a45b75a5442622 and a total number of 953 components indexed\n",
      "2024-02-14 19:47:44,782 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-02-14 19:47:44,782 - modelscope - INFO - Use user-specified model revision: master\n",
      "Downloading: 100%|█████████████████████████| 8.21k/8.21k [00:00<00:00, 45.8MB/s]\n",
      "Downloading: 100%|█████████████████████████| 50.8k/50.8k [00:00<00:00, 50.0MB/s]\n",
      "Downloading: 100%|███████████████████████████| 244k/244k [00:00<00:00, 13.7MB/s]\n",
      "Downloading: 100%|███████████████████████████| 135k/135k [00:00<00:00, 13.4MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 910/910 [00:00<00:00, 10.1MB/s]\n",
      "Downloading: 100%|████████████████████████████| 88.0/88.0 [00:00<00:00, 918kB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.29k/2.29k [00:00<00:00, 20.9MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.88k/1.88k [00:00<00:00, 20.6MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 222/222 [00:00<00:00, 2.50MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.63M/1.63M [00:00<00:00, 59.2MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.84M/1.84M [00:00<00:00, 67.4MB/s]\n",
      "Downloading: 100%|██████████████████████████| 2.64M/2.64M [00:00<00:00, 102MB/s]\n",
      "Downloading: 100%|█████████████████████████| 6.73k/6.73k [00:00<00:00, 6.53MB/s]\n",
      "Downloading: 100%|█████████████████████████| 80.8k/80.8k [00:00<00:00, 7.46MB/s]\n",
      "Downloading: 100%|█████████████████████████| 80.8k/80.8k [00:00<00:00, 21.1MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.83G/1.83G [00:05<00:00, 389MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:05<00:00, 350MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:05<00:00, 363MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:06<00:00, 300MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:05<00:00, 351MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:05<00:00, 346MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.88G/1.88G [00:05<00:00, 346MB/s]\n",
      "Downloading: 100%|█████████████████████████▉| 1.24G/1.24G [00:03<00:00, 375MB/s]\n",
      "Downloading: 100%|█████████████████████████| 19.1k/19.1k [00:00<00:00, 9.98MB/s]\n",
      "Downloading: 100%|█████████████████████████| 54.3k/54.3k [00:00<00:00, 4.86MB/s]\n",
      "Downloading: 100%|█████████████████████████| 15.0k/15.0k [00:00<00:00, 33.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 237k/237k [00:00<00:00, 73.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 116k/116k [00:00<00:00, 15.1MB/s]\n",
      "Downloading: 100%|█████████████████████████| 2.44M/2.44M [00:00<00:00, 95.3MB/s]\n",
      "Downloading: 100%|███████████████████████████| 473k/473k [00:00<00:00, 16.3MB/s]\n",
      "Downloading: 100%|█████████████████████████| 14.3k/14.3k [00:00<00:00, 90.1MB/s]\n",
      "Downloading: 100%|█████████████████████████| 79.0k/79.0k [00:00<00:00, 11.1MB/s]\n",
      "Downloading: 100%|█████████████████████████| 46.4k/46.4k [00:00<00:00, 33.3MB/s]\n",
      "Downloading: 100%|█████████████████████████| 0.98M/0.98M [00:00<00:00, 19.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 205k/205k [00:00<00:00, 22.6MB/s]\n",
      "Downloading: 100%|█████████████████████████| 19.4k/19.4k [00:00<00:00, 5.54MB/s]\n",
      "Downloading: 100%|███████████████████████████| 302k/302k [00:00<00:00, 85.4MB/s]\n",
      "Downloading: 100%|███████████████████████████| 615k/615k [00:00<00:00, 54.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 376k/376k [00:00<00:00, 17.3MB/s]\n",
      "Downloading: 100%|███████████████████████████| 445k/445k [00:00<00:00, 16.4MB/s]\n",
      "Downloading: 100%|█████████████████████████| 19.5k/19.5k [00:00<00:00, 5.43MB/s]\n",
      "Downloading: 100%|███████████████████████████| 395k/395k [00:00<00:00, 25.8MB/s]\n",
      "Downloading: 100%|███████████████████████████| 176k/176k [00:00<00:00, 11.4MB/s]\n",
      "Downloading: 100%|███████████████████████████| 182k/182k [00:00<00:00, 13.2MB/s]\n",
      "Downloading: 100%|███████████████████████████| 824k/824k [00:00<00:00, 17.3MB/s]\n",
      "Downloading: 100%|███████████████████████████| 426k/426k [00:00<00:00, 21.6MB/s]\n",
      "Downloading: 100%|███████████████████████████| 433k/433k [00:00<00:00, 21.6MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 47.5MB/s]\n",
      "Downloading: 100%|███████████████████████████| 403k/403k [00:00<00:00, 33.1MB/s]\n",
      "Downloading: 100%|█████████████████████████| 9.39k/9.39k [00:00<00:00, 59.7MB/s]\n",
      "Downloading: 100%|███████████████████████████| 403k/403k [00:00<00:00, 20.3MB/s]\n",
      "Downloading: 100%|█████████████████████████| 79.0k/79.0k [00:00<00:00, 15.6MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 173/173 [00:00<00:00, 1.51MB/s]\n",
      "Downloading: 100%|█████████████████████████| 41.9k/41.9k [00:00<00:00, 5.22MB/s]\n",
      "Downloading: 100%|███████████████████████████| 230k/230k [00:00<00:00, 14.2MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.27M/1.27M [00:00<00:00, 73.1MB/s]\n",
      "Downloading: 100%|███████████████████████████| 664k/664k [00:00<00:00, 30.9MB/s]\n",
      "Downloading: 100%|███████████████████████████| 404k/404k [00:00<00:00, 19.4MB/s]\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:49:04,394 >> loading file qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:49:04,394 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:49:04,394 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:49:04,394 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:49:04,394 >> loading file tokenizer.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:49:04,880 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:49:04,882 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-14 19:49:04,883 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/qwen/Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-14 19:49:04,928 >> loading weights file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1426] 2024-02-14 19:49:04,928 >> Instantiating QWenLMHeadModel model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:49:04,929 >> Generate config GenerationConfig {}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:01<00:00,  4.10it/s]\n",
      "[INFO|modeling_utils.py:4350] 2024-02-14 19:49:07,241 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-14 19:49:07,241 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/qwen/Qwen-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:779] 2024-02-14 19:49:07,243 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:49:07,244 >> Generate config GenerationConfig {\n",
      "  \"chat_format\": \"raw\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"stop_words_ids\": [\n",
      "    [\n",
      "      151643\n",
      "    ]\n",
      "  ],\n",
      "  \"top_k\": 0,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 7725518848 || trainable%: 0.0543\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
      "02/14/2024 19:49:07 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
      "02/14/2024 19:49:07 - INFO - llmtuner.data.loader - Loading dataset train_data_for_Qwen.json...\n",
      "02/14/2024 19:49:07 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Using custom data configuration default-a07d5e2a5227c62d\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 40000 examples [00:02, 14156.74 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "Converting format of dataset:   0%|            | 0/40000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b1ddfd5e01870c8.arrow\n",
      "Converting format of dataset: 100%|█| 40000/40000 [00:01<00:00, 32305.13 example\n",
      "Running tokenizer on dataset:   0%|            | 0/40000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43022c9f4fef86fa.arrow\n",
      "Running tokenizer on dataset: 100%|█| 40000/40000 [00:55<00:00, 717.48 examples/\n",
      "input_ids:\n",
      "[33975, 25, 715, 262, 1446, 525, 264, 26339, 6203, 13, 16246, 458, 1946, 3405, 11, 1156, 1855, 264, 83187, 531, 2673, 4396, 7870, 198, 262, 3239, 311, 1598, 11, 1221, 1401, 518, 279, 3059, 315, 279, 3239, 323, 470, 279, 4226, 311, 279, 1946, 3405, 624, 262, 1446, 1969, 3239, 1172, 279, 8147, 429, 525, 4362, 311, 4226, 279, 3405, 624, 262, 11471, 6529, 311, 990, 1172, 279, 3250, 5036, 498, 646, 1490, 304, 279, 12632, 3685, 13, 2823, 16585, 311, 537, 3239, 369, 8147, 429, 653, 537, 3000, 624, 262, 2291, 6529, 311, 892, 8147, 374, 304, 892, 1965, 624, 262, 11471, 6529, 311, 429, 279, 21568, 7332, 525, 1142, 16216, 323, 1969, 2432, 279, 8147, 829, 624, 262, 11471, 6529, 311, 470, 458, 32156, 5704, 3239, 624, 262, 11471, 6529, 311, 429, 279, 2750, 315, 7332, 1184, 311, 387, 43810, 304, 54231, 15423, 11, 369, 3110, 25, 432, 1265, 387, 364, 19094, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 198, 262, 5288, 1375, 62, 22, 366, 39022, 364, 4518, 315, 364, 4858, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 5288, 1375, 62, 22, 366, 330, 13683, 20584, 624, 262, 11471, 6529, 311, 429, 7870, 12632, 1184, 311, 387, 2661, 40386, 979, 1667, 5138, 11, 369, 3110, 25, 432, 1265, 387, 364, 4858, 264, 2644, 4295, 12089, 5752, 264, 13069, 6311, 5752, 272, 6197, 264, 13, 3779, 284, 272, 13, 3779, 13069, 198, 262, 5700, 5752, 296, 6197, 272, 744, 21027, 284, 296, 48071, 5288, 296, 6067, 284, 364, 115560, 62922, 107309, 1, 3567, 272, 26006, 284, 364, 38507, 103250, 99741, 4605, 4518, 315, 364, 4858, 12089, 2644, 4295, 12089, 13069, 6311, 6197, 12089, 13, 3779, 284, 6311, 13, 3779, 13069, 198, 262, 5700, 6197, 6311, 744, 21027, 284, 5700, 48071, 5288, 5700, 6067, 284, 364, 115560, 62922, 107309, 6, 3567, 6311, 26006, 284, 364, 38507, 103250, 99741, 4605, 382, 262, 9258, 279, 1590, 7870, 3239, 1172, 624, 1066, 262, 5443, 279, 2701, 3561, 510, 262, 7870, 2859, 25, 220, 7870, 11361, 1588, 624, 262, 8278, 990, 279, 2701, 12632, 510, 1066, 310, 30776, 14363, 1968, 320, 1983, 842, 1372, 11, 829, 1467, 11, 9223, 4387, 1467, 11, 4231, 1372, 317, 310, 1391, 785, 1965, 1968, 4008, 374, 25, 364, 1983, 23569, 310, 576, 1968, 842, 2070, 315, 1968, 3363, 1968, 877, 323, 702, 3204, 2750, 438, 25, 2509, 16, 516, 364, 16, 15, 516, 364, 22, 516, 364, 21, 516, 364, 23, 660, 624, 785, 829, 2070, 315, 1968, 3363, 829, 323, 702, 3204, 2750, 438, 25, 2509, 42, 13, 619, 13, 86573, 516, 364, 37326, 3732, 22463, 5604, 516, 364, 50, 2375, 815, 84241, 516, 364, 623, 26497, 356, 766, 516, 364, 30356, 22869, 2982, 660, 624, 785, 9223, 4387, 2070, 315, 1968, 3363, 9223, 1584, 323, 702, 3204, 2750, 438, 25, 2509, 45410, 516, 364, 16532, 19541, 516, 364, 14611, 27306, 516, 364, 57027, 516, 364, 97811, 660, 624, 785, 4231, 2070, 315, 1968, 3363, 4231, 323, 702, 3204, 2750, 438, 25, 2509, 20, 21, 13, 15, 516, 364, 20, 18, 13, 15, 516, 364, 20, 17, 13, 15, 516, 364, 21, 23, 13, 15, 516, 364, 20, 15, 13, 15, 660, 382, 310, 735, 286, 330, 14582, 788, 18137, 225, 101, 64689, 105656, 100430, 17340, 102185, 107043, 20, 21, 92015, 94432, 286, 330, 6688, 2859, 788, 19094, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 8945, 71703, 25, 220, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "inputs:\n",
      "Human: \n",
      "    You are a MySQL expert. Given an input question, first create a syntactically correct SQL\n",
      "    query to run, then look at the results of the query and return the answer to the input question.\n",
      "    You must query only the columns that are needed to answer the question.\n",
      "    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist.\n",
      "    pay attention to which columns is in which table.\n",
      "    Pay attention to that the constraint variables are case sensitive and must match the columns name.\n",
      "    Pay attention to return an executable sql query.\n",
      "    Pay attention to that the values of variables need to be enclosed in quotation marks, for example: it should be ' SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445\n",
      "    WHERE col_7 < abc ' instead of 'SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445 WHERE col_7 < \"abc\"'.\n",
      "    Pay attention to that SQL tables need to be given aliases when using join, for example: it should be 'SELECT a.name FROM actor AS a JOIN cast AS c ON a.aid = c.aid JOIN\n",
      "    movie AS m ON c.msid = m.mid WHERE m.title = '霸王别姬\" AND c.role = '程蝶衣'' instead of 'SELECT actor.name FROM actor JOIN cast ON actor.aid = cast.aid JOIN\n",
      "    movie ON cast.msid = movie.mid WHERE movie.title = '霸王别姬' AND cast.role = '程蝶衣''.\n",
      "\n",
      "    Output the final SQL query only.\n",
      "    \n",
      "    Use the following format:\n",
      "    SQLQuery:  SQL Query here.\n",
      "    Only use the following tables:\n",
      "    \n",
      "            CREATE TABLE head (head_id number, name text, born_state text, age number);\n",
      "            /*The table head description is: 'head'.\n",
      "            The head_id field of head means head id and has possible values as: ['1', '10', '7', '6', '8'].\n",
      "The name field of head means name and has possible values as: ['K. J. Choi', 'Franklin Langham', 'Sergio García', 'Stewart Cink', 'Nick Faldo'].\n",
      "The born_state field of head means born state and has possible values as: ['California', 'Delaware', 'Connecticut', 'Florida', 'Alabama'].\n",
      "The age field of head means age and has possible values as: ['56.0', '53.0', '52.0', '68.0', '50.0'].\n",
      "\n",
      "            */\n",
      "        \"Question\": 部门中有多少人年龄大于56岁？\n",
      "        \"SQLQuery\": SELECT count(*) FROM head WHERE age  >  56\n",
      "        \n",
      "Assistant: SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "labels:\n",
      "SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:50:09,608 >> PyTorch: setting up devices\n",
      "[INFO|trainer.py:571] 2024-02-14 19:50:53,848 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1721] 2024-02-14 19:50:54,095 >> ***** Running training *****\n",
      "[INFO|trainer.py:1722] 2024-02-14 19:50:54,095 >>   Num examples = 40,000\n",
      "[INFO|trainer.py:1723] 2024-02-14 19:50:54,095 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1724] 2024-02-14 19:50:54,095 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1727] 2024-02-14 19:50:54,095 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:1728] 2024-02-14 19:50:54,095 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1729] 2024-02-14 19:50:54,095 >>   Total optimization steps = 7,500\n",
      "[INFO|trainer.py:1730] 2024-02-14 19:50:54,096 >>   Number of trainable parameters = 4,194,304\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-8 (run_exp):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 31, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 75, in run_sft\n",
      "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2772, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2795, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 680, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 668, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1073, in forward\n",
      "    return self.base_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 103, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py\", line 1068, in forward\n",
      "    loss = loss_fct(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 1179, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 3053, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacty of 22.20 GiB of which 1.41 GiB is free. Process 205794 has 20.79 GiB memory in use. Of the allocated memory 19.05 GiB is allocated by PyTorch, and 318.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:51:53,258 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1566] 2024-02-14 19:51:53,259 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "02/14/2024 19:51:53 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:51:53,266 >> PyTorch: setting up devices\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1741: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "Exception in thread Thread-11 (run_exp):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 25, in run_exp\n",
      "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 192, in get_train_args\n",
      "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
      "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:52:22,657 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1566] 2024-02-14 19:52:22,657 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "02/14/2024 19:52:22 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:52:22,663 >> PyTorch: setting up devices\n",
      "02/14/2024 19:52:22 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "02/14/2024 19:52:22 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=saves/Qwen-7B/lora/king/runs/Feb14_19-52-22_dsw-288938-6d9f6bc866-5wj5n,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=saves/Qwen-7B/lora/king,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=3,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=saves/Qwen-7B/lora/king,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-02-14 19:52:23,029 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-02-14 19:52:23,029 - modelscope - INFO - Use user-specified model revision: master\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:52:23,779 >> loading file qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:52:23,779 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:52:23,779 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:52:23,779 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:52:23,779 >> loading file tokenizer.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:52:24,030 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:52:24,030 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-14 19:52:24,031 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/qwen/Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-14 19:52:24,036 >> loading weights file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1426] 2024-02-14 19:52:24,037 >> Instantiating QWenLMHeadModel model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:52:24,037 >> Generate config GenerationConfig {}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:01<00:00,  4.17it/s]\n",
      "[INFO|modeling_utils.py:4350] 2024-02-14 19:52:26,276 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-14 19:52:26,277 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/qwen/Qwen-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:779] 2024-02-14 19:52:26,279 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:52:26,279 >> Generate config GenerationConfig {\n",
      "  \"chat_format\": \"raw\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"stop_words_ids\": [\n",
      "    [\n",
      "      151643\n",
      "    ]\n",
      "  ],\n",
      "  \"top_k\": 0,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 7725518848 || trainable%: 0.0543\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
      "02/14/2024 19:52:26 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
      "02/14/2024 19:52:26 - INFO - llmtuner.data.loader - Loading dataset train_data_for_Qwen.json...\n",
      "02/14/2024 19:52:26 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Using custom data configuration default-a07d5e2a5227c62d\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b1ddfd5e01870c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43022c9f4fef86fa.arrow\n",
      "input_ids:\n",
      "[33975, 25, 715, 262, 1446, 525, 264, 26339, 6203, 13, 16246, 458, 1946, 3405, 11, 1156, 1855, 264, 83187, 531, 2673, 4396, 7870, 198, 262, 3239, 311, 1598, 11, 1221, 1401, 518, 279, 3059, 315, 279, 3239, 323, 470, 279, 4226, 311, 279, 1946, 3405, 624, 262, 1446, 1969, 3239, 1172, 279, 8147, 429, 525, 4362, 311, 4226, 279, 3405, 624, 262, 11471, 6529, 311, 990, 1172, 279, 3250, 5036, 498, 646, 1490, 304, 279, 12632, 3685, 13, 2823, 16585, 311, 537, 3239, 369, 8147, 429, 653, 537, 3000, 624, 262, 2291, 6529, 311, 892, 8147, 374, 304, 892, 1965, 624, 262, 11471, 6529, 311, 429, 279, 21568, 7332, 525, 1142, 16216, 323, 1969, 2432, 279, 8147, 829, 624, 262, 11471, 6529, 311, 470, 458, 32156, 5704, 3239, 624, 262, 11471, 6529, 311, 429, 279, 2750, 315, 7332, 1184, 311, 387, 43810, 304, 54231, 15423, 11, 369, 3110, 25, 432, 1265, 387, 364, 19094, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 198, 262, 5288, 1375, 62, 22, 366, 39022, 364, 4518, 315, 364, 4858, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 5288, 1375, 62, 22, 366, 330, 13683, 20584, 624, 262, 11471, 6529, 311, 429, 7870, 12632, 1184, 311, 387, 2661, 40386, 979, 1667, 5138, 11, 369, 3110, 25, 432, 1265, 387, 364, 4858, 264, 2644, 4295, 12089, 5752, 264, 13069, 6311, 5752, 272, 6197, 264, 13, 3779, 284, 272, 13, 3779, 13069, 198, 262, 5700, 5752, 296, 6197, 272, 744, 21027, 284, 296, 48071, 5288, 296, 6067, 284, 364, 115560, 62922, 107309, 1, 3567, 272, 26006, 284, 364, 38507, 103250, 99741, 4605, 4518, 315, 364, 4858, 12089, 2644, 4295, 12089, 13069, 6311, 6197, 12089, 13, 3779, 284, 6311, 13, 3779, 13069, 198, 262, 5700, 6197, 6311, 744, 21027, 284, 5700, 48071, 5288, 5700, 6067, 284, 364, 115560, 62922, 107309, 6, 3567, 6311, 26006, 284, 364, 38507, 103250, 99741, 4605, 382, 262, 9258, 279, 1590, 7870, 3239, 1172, 624, 1066, 262, 5443, 279, 2701, 3561, 510, 262, 7870, 2859, 25, 220, 7870, 11361, 1588, 624, 262, 8278, 990, 279, 2701, 12632, 510, 1066, 310, 30776, 14363, 1968, 320, 1983, 842, 1372, 11, 829, 1467, 11, 9223, 4387, 1467, 11, 4231, 1372, 317, 310, 1391, 785, 1965, 1968, 4008, 374, 25, 364, 1983, 23569, 310, 576, 1968, 842, 2070, 315, 1968, 3363, 1968, 877, 323, 702, 3204, 2750, 438, 25, 2509, 16, 516, 364, 16, 15, 516, 364, 22, 516, 364, 21, 516, 364, 23, 660, 624, 785, 829, 2070, 315, 1968, 3363, 829, 323, 702, 3204, 2750, 438, 25, 2509, 42, 13, 619, 13, 86573, 516, 364, 37326, 3732, 22463, 5604, 516, 364, 50, 2375, 815, 84241, 516, 364, 623, 26497, 356, 766, 516, 364, 30356, 22869, 2982, 660, 624, 785, 9223, 4387, 2070, 315, 1968, 3363, 9223, 1584, 323, 702, 3204, 2750, 438, 25, 2509, 45410, 516, 364, 16532, 19541, 516, 364, 14611, 27306, 516, 364, 57027, 516, 364, 97811, 660, 624, 785, 4231, 2070, 315, 1968, 3363, 4231, 323, 702, 3204, 2750, 438, 25, 2509, 20, 21, 13, 15, 516, 364, 20, 18, 13, 15, 516, 364, 20, 17, 13, 15, 516, 364, 21, 23, 13, 15, 516, 364, 20, 15, 13, 15, 660, 382, 310, 735, 286, 330, 14582, 788, 18137, 225, 101, 64689, 105656, 100430, 17340, 102185, 107043, 20, 21, 92015, 94432, 286, 330, 6688, 2859, 788, 19094, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 8945, 71703, 25, 220, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "inputs:\n",
      "Human: \n",
      "    You are a MySQL expert. Given an input question, first create a syntactically correct SQL\n",
      "    query to run, then look at the results of the query and return the answer to the input question.\n",
      "    You must query only the columns that are needed to answer the question.\n",
      "    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist.\n",
      "    pay attention to which columns is in which table.\n",
      "    Pay attention to that the constraint variables are case sensitive and must match the columns name.\n",
      "    Pay attention to return an executable sql query.\n",
      "    Pay attention to that the values of variables need to be enclosed in quotation marks, for example: it should be ' SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445\n",
      "    WHERE col_7 < abc ' instead of 'SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445 WHERE col_7 < \"abc\"'.\n",
      "    Pay attention to that SQL tables need to be given aliases when using join, for example: it should be 'SELECT a.name FROM actor AS a JOIN cast AS c ON a.aid = c.aid JOIN\n",
      "    movie AS m ON c.msid = m.mid WHERE m.title = '霸王别姬\" AND c.role = '程蝶衣'' instead of 'SELECT actor.name FROM actor JOIN cast ON actor.aid = cast.aid JOIN\n",
      "    movie ON cast.msid = movie.mid WHERE movie.title = '霸王别姬' AND cast.role = '程蝶衣''.\n",
      "\n",
      "    Output the final SQL query only.\n",
      "    \n",
      "    Use the following format:\n",
      "    SQLQuery:  SQL Query here.\n",
      "    Only use the following tables:\n",
      "    \n",
      "            CREATE TABLE head (head_id number, name text, born_state text, age number);\n",
      "            /*The table head description is: 'head'.\n",
      "            The head_id field of head means head id and has possible values as: ['1', '10', '7', '6', '8'].\n",
      "The name field of head means name and has possible values as: ['K. J. Choi', 'Franklin Langham', 'Sergio García', 'Stewart Cink', 'Nick Faldo'].\n",
      "The born_state field of head means born state and has possible values as: ['California', 'Delaware', 'Connecticut', 'Florida', 'Alabama'].\n",
      "The age field of head means age and has possible values as: ['56.0', '53.0', '52.0', '68.0', '50.0'].\n",
      "\n",
      "            */\n",
      "        \"Question\": 部门中有多少人年龄大于56岁？\n",
      "        \"SQLQuery\": SELECT count(*) FROM head WHERE age  >  56\n",
      "        \n",
      "Assistant: SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "labels:\n",
      "SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:52:28,923 >> PyTorch: setting up devices\n",
      "[INFO|trainer.py:571] 2024-02-14 19:53:10,057 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1721] 2024-02-14 19:53:10,310 >> ***** Running training *****\n",
      "[INFO|trainer.py:1722] 2024-02-14 19:53:10,310 >>   Num examples = 40,000\n",
      "[INFO|trainer.py:1723] 2024-02-14 19:53:10,310 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1724] 2024-02-14 19:53:10,310 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:1727] 2024-02-14 19:53:10,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1728] 2024-02-14 19:53:10,310 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1729] 2024-02-14 19:53:10,310 >>   Total optimization steps = 9,999\n",
      "[INFO|trainer.py:1730] 2024-02-14 19:53:10,311 >>   Number of trainable parameters = 4,194,304\n",
      "Exception in thread Thread-12 (run_exp):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 31, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 75, in run_sft\n",
      "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2772, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2795, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 680, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 668, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 647, in convert_to_fp32\n",
      "    return recursively_apply(_convert_to_fp32, tensor, test_type=_is_fp16_bf16_tensor)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 121, in recursively_apply\n",
      "    {\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 122, in <dictcomp>\n",
      "    k: recursively_apply(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 129, in recursively_apply\n",
      "    return func(data, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 642, in _convert_to_fp32\n",
      "    return tensor.float()\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacty of 22.20 GiB of which 1.59 GiB is free. Process 205794 has 20.61 GiB memory in use. Of the allocated memory 18.73 GiB is allocated by PyTorch, and 461.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:53:34,604 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1566] 2024-02-14 19:53:34,604 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "02/14/2024 19:53:34 - WARNING - llmtuner.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:53:34,610 >> PyTorch: setting up devices\n",
      "02/14/2024 19:53:34 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
      "  distributed training: True, compute dtype: torch.bfloat16\n",
      "02/14/2024 19:53:34 - INFO - llmtuner.hparams.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=True,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=saves/Qwen-7B/lora/king_007/runs/Feb14_19-53-34_dsw-288938-6d9f6bc866-5wj5n,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=saves/Qwen-7B/lora/king_007,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=2,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=saves/Qwen-7B/lora/king_007,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-02-14 19:53:34,958 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-02-14 19:53:34,958 - modelscope - INFO - Use user-specified model revision: master\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:53:35,573 >> loading file qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:53:35,574 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:53:35,574 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:53:35,574 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-14 19:53:35,574 >> loading file tokenizer.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:53:35,818 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-14 19:53:35,819 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-14 19:53:35,820 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/qwen/Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-14 19:53:35,825 >> loading weights file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1426] 2024-02-14 19:53:35,825 >> Instantiating QWenLMHeadModel model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:53:35,826 >> Generate config GenerationConfig {}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:01<00:00,  4.14it/s]\n",
      "[INFO|modeling_utils.py:4350] 2024-02-14 19:53:38,114 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-14 19:53:38,114 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/qwen/Qwen-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:779] 2024-02-14 19:53:38,116 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-14 19:53:38,116 >> Generate config GenerationConfig {\n",
      "  \"chat_format\": \"raw\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"stop_words_ids\": [\n",
      "    [\n",
      "      151643\n",
      "    ]\n",
      "  ],\n",
      "  \"top_k\": 0,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 7725518848 || trainable%: 0.0543\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
      "02/14/2024 19:53:38 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
      "02/14/2024 19:53:38 - INFO - llmtuner.data.loader - Loading dataset train_data_for_Qwen.json...\n",
      "02/14/2024 19:53:38 - WARNING - llmtuner.data.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.\n",
      "Using custom data configuration default-a07d5e2a5227c62d\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8b1ddfd5e01870c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-a07d5e2a5227c62d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-43022c9f4fef86fa.arrow\n",
      "input_ids:\n",
      "[33975, 25, 715, 262, 1446, 525, 264, 26339, 6203, 13, 16246, 458, 1946, 3405, 11, 1156, 1855, 264, 83187, 531, 2673, 4396, 7870, 198, 262, 3239, 311, 1598, 11, 1221, 1401, 518, 279, 3059, 315, 279, 3239, 323, 470, 279, 4226, 311, 279, 1946, 3405, 624, 262, 1446, 1969, 3239, 1172, 279, 8147, 429, 525, 4362, 311, 4226, 279, 3405, 624, 262, 11471, 6529, 311, 990, 1172, 279, 3250, 5036, 498, 646, 1490, 304, 279, 12632, 3685, 13, 2823, 16585, 311, 537, 3239, 369, 8147, 429, 653, 537, 3000, 624, 262, 2291, 6529, 311, 892, 8147, 374, 304, 892, 1965, 624, 262, 11471, 6529, 311, 429, 279, 21568, 7332, 525, 1142, 16216, 323, 1969, 2432, 279, 8147, 829, 624, 262, 11471, 6529, 311, 470, 458, 32156, 5704, 3239, 624, 262, 11471, 6529, 311, 429, 279, 2750, 315, 7332, 1184, 311, 387, 43810, 304, 54231, 15423, 11, 369, 3110, 25, 432, 1265, 387, 364, 19094, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 198, 262, 5288, 1375, 62, 22, 366, 39022, 364, 4518, 315, 364, 4858, 1375, 62, 16, 4295, 6633, 62, 21, 24, 67, 18, 68, 19, 20, 19, 18, 18, 19, 18, 16, 16, 68, 24, 23, 18, 16, 68, 20, 19, 17, 21, 24, 21, 67, 21, 68, 19, 19, 20, 5288, 1375, 62, 22, 366, 330, 13683, 20584, 624, 262, 11471, 6529, 311, 429, 7870, 12632, 1184, 311, 387, 2661, 40386, 979, 1667, 5138, 11, 369, 3110, 25, 432, 1265, 387, 364, 4858, 264, 2644, 4295, 12089, 5752, 264, 13069, 6311, 5752, 272, 6197, 264, 13, 3779, 284, 272, 13, 3779, 13069, 198, 262, 5700, 5752, 296, 6197, 272, 744, 21027, 284, 296, 48071, 5288, 296, 6067, 284, 364, 115560, 62922, 107309, 1, 3567, 272, 26006, 284, 364, 38507, 103250, 99741, 4605, 4518, 315, 364, 4858, 12089, 2644, 4295, 12089, 13069, 6311, 6197, 12089, 13, 3779, 284, 6311, 13, 3779, 13069, 198, 262, 5700, 6197, 6311, 744, 21027, 284, 5700, 48071, 5288, 5700, 6067, 284, 364, 115560, 62922, 107309, 6, 3567, 6311, 26006, 284, 364, 38507, 103250, 99741, 4605, 382, 262, 9258, 279, 1590, 7870, 3239, 1172, 624, 1066, 262, 5443, 279, 2701, 3561, 510, 262, 7870, 2859, 25, 220, 7870, 11361, 1588, 624, 262, 8278, 990, 279, 2701, 12632, 510, 1066, 310, 30776, 14363, 1968, 320, 1983, 842, 1372, 11, 829, 1467, 11, 9223, 4387, 1467, 11, 4231, 1372, 317, 310, 1391, 785, 1965, 1968, 4008, 374, 25, 364, 1983, 23569, 310, 576, 1968, 842, 2070, 315, 1968, 3363, 1968, 877, 323, 702, 3204, 2750, 438, 25, 2509, 16, 516, 364, 16, 15, 516, 364, 22, 516, 364, 21, 516, 364, 23, 660, 624, 785, 829, 2070, 315, 1968, 3363, 829, 323, 702, 3204, 2750, 438, 25, 2509, 42, 13, 619, 13, 86573, 516, 364, 37326, 3732, 22463, 5604, 516, 364, 50, 2375, 815, 84241, 516, 364, 623, 26497, 356, 766, 516, 364, 30356, 22869, 2982, 660, 624, 785, 9223, 4387, 2070, 315, 1968, 3363, 9223, 1584, 323, 702, 3204, 2750, 438, 25, 2509, 45410, 516, 364, 16532, 19541, 516, 364, 14611, 27306, 516, 364, 57027, 516, 364, 97811, 660, 624, 785, 4231, 2070, 315, 1968, 3363, 4231, 323, 702, 3204, 2750, 438, 25, 2509, 20, 21, 13, 15, 516, 364, 20, 18, 13, 15, 516, 364, 20, 17, 13, 15, 516, 364, 21, 23, 13, 15, 516, 364, 20, 15, 13, 15, 660, 382, 310, 735, 286, 330, 14582, 788, 18137, 225, 101, 64689, 105656, 100430, 17340, 102185, 107043, 20, 21, 92015, 94432, 286, 330, 6688, 2859, 788, 19094, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 8945, 71703, 25, 220, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "inputs:\n",
      "Human: \n",
      "    You are a MySQL expert. Given an input question, first create a syntactically correct SQL\n",
      "    query to run, then look at the results of the query and return the answer to the input question.\n",
      "    You must query only the columns that are needed to answer the question.\n",
      "    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist.\n",
      "    pay attention to which columns is in which table.\n",
      "    Pay attention to that the constraint variables are case sensitive and must match the columns name.\n",
      "    Pay attention to return an executable sql query.\n",
      "    Pay attention to that the values of variables need to be enclosed in quotation marks, for example: it should be ' SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445\n",
      "    WHERE col_7 < abc ' instead of 'SELECT col_1 FROM Table_69d3e454334311e9831e542696d6e445 WHERE col_7 < \"abc\"'.\n",
      "    Pay attention to that SQL tables need to be given aliases when using join, for example: it should be 'SELECT a.name FROM actor AS a JOIN cast AS c ON a.aid = c.aid JOIN\n",
      "    movie AS m ON c.msid = m.mid WHERE m.title = '霸王别姬\" AND c.role = '程蝶衣'' instead of 'SELECT actor.name FROM actor JOIN cast ON actor.aid = cast.aid JOIN\n",
      "    movie ON cast.msid = movie.mid WHERE movie.title = '霸王别姬' AND cast.role = '程蝶衣''.\n",
      "\n",
      "    Output the final SQL query only.\n",
      "    \n",
      "    Use the following format:\n",
      "    SQLQuery:  SQL Query here.\n",
      "    Only use the following tables:\n",
      "    \n",
      "            CREATE TABLE head (head_id number, name text, born_state text, age number);\n",
      "            /*The table head description is: 'head'.\n",
      "            The head_id field of head means head id and has possible values as: ['1', '10', '7', '6', '8'].\n",
      "The name field of head means name and has possible values as: ['K. J. Choi', 'Franklin Langham', 'Sergio García', 'Stewart Cink', 'Nick Faldo'].\n",
      "The born_state field of head means born state and has possible values as: ['California', 'Delaware', 'Connecticut', 'Florida', 'Alabama'].\n",
      "The age field of head means age and has possible values as: ['56.0', '53.0', '52.0', '68.0', '50.0'].\n",
      "\n",
      "            */\n",
      "        \"Question\": 部门中有多少人年龄大于56岁？\n",
      "        \"SQLQuery\": SELECT count(*) FROM head WHERE age  >  56\n",
      "        \n",
      "Assistant: SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4858, 1760, 28671, 4295, 1968, 5288, 4231, 220, 861, 220, 220, 20, 21, 151643]\n",
      "labels:\n",
      "SELECT count(*) FROM head WHERE age  >  56<|endoftext|>\n",
      "[INFO|training_args.py:1828] 2024-02-14 19:53:40,593 >> PyTorch: setting up devices\n",
      "[INFO|trainer.py:571] 2024-02-14 19:54:21,685 >> Using auto half precision backend\n",
      "[INFO|trainer.py:1721] 2024-02-14 19:54:21,937 >> ***** Running training *****\n",
      "[INFO|trainer.py:1722] 2024-02-14 19:54:21,937 >>   Num examples = 40,000\n",
      "[INFO|trainer.py:1723] 2024-02-14 19:54:21,937 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1724] 2024-02-14 19:54:21,937 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:1727] 2024-02-14 19:54:21,937 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1728] 2024-02-14 19:54:21,937 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1729] 2024-02-14 19:54:21,937 >>   Total optimization steps = 15,000\n",
      "[INFO|trainer.py:1730] 2024-02-14 19:54:21,938 >>   Number of trainable parameters = 4,194,304\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 19:54:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.7700, 'learning_rate': 5.0000e-05, 'epoch': 0.00}\n",
      "{'loss': 0.77, 'learning_rate': 4.9999986292217365e-05, 'epoch': 0.0}\n",
      "02/14/2024 19:55:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.8374, 'learning_rate': 5.0000e-05, 'epoch': 0.00}\n",
      "{'loss': 0.8374, 'learning_rate': 4.999994516888449e-05, 'epoch': 0.0}\n",
      "02/14/2024 19:56:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.6797, 'learning_rate': 5.0000e-05, 'epoch': 0.00}\n",
      "{'loss': 0.6797, 'learning_rate': 4.999987663004646e-05, 'epoch': 0.0}\n",
      "02/14/2024 19:56:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.6546, 'learning_rate': 5.0000e-05, 'epoch': 0.00}\n",
      "{'loss': 0.6546, 'learning_rate': 4.999978067577844e-05, 'epoch': 0.0}\n",
      "02/14/2024 19:57:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.6220, 'learning_rate': 5.0000e-05, 'epoch': 0.01}\n",
      "{'loss': 0.622, 'learning_rate': 4.999965730618567e-05, 'epoch': 0.01}\n",
      "02/14/2024 19:58:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.4233, 'learning_rate': 5.0000e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4233, 'learning_rate': 4.999950652140343e-05, 'epoch': 0.01}\n",
      "02/14/2024 19:58:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.4028, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4028, 'learning_rate': 4.999932832159707e-05, 'epoch': 0.01}\n",
      "02/14/2024 19:59:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.4196, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4196, 'learning_rate': 4.999912270696202e-05, 'epoch': 0.01}\n",
      "02/14/2024 19:59:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.2706, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
      "{'loss': 0.2706, 'learning_rate': 4.999888967772375e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:00:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.3197, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3197, 'learning_rate': 4.999862923413781e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:01:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.2684, 'learning_rate': 4.9998e-05, 'epoch': 0.01}\n",
      "{'loss': 0.2684, 'learning_rate': 4.99983413764898e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:01:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.2627, 'learning_rate': 4.9998e-05, 'epoch': 0.01}\n",
      "{'loss': 0.2627, 'learning_rate': 4.9998026105095405e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:02:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.2439, 'learning_rate': 4.9998e-05, 'epoch': 0.01}\n",
      "{'loss': 0.2439, 'learning_rate': 4.999768342030035e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:03:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1776, 'learning_rate': 4.9997e-05, 'epoch': 0.01}\n",
      "{'loss': 0.1776, 'learning_rate': 4.999731332248044e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:03:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.2004, 'learning_rate': 4.9997e-05, 'epoch': 0.01}\n",
      "{'loss': 0.2004, 'learning_rate': 4.999691581204152e-05, 'epoch': 0.01}\n",
      "02/14/2024 20:04:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.1927, 'learning_rate': 4.9996e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1927, 'learning_rate': 4.9996490889419514e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:04:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.1455, 'learning_rate': 4.9996e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1455, 'learning_rate': 4.999603855508041e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:05:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1781, 'learning_rate': 4.9996e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1781, 'learning_rate': 4.999555880952023e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:06:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.1269, 'learning_rate': 4.9995e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1269, 'learning_rate': 4.999505165326509e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:06:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1885, 'learning_rate': 4.9995e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1885, 'learning_rate': 4.999451708687114e-05, 'epoch': 0.02}\n",
      "[INFO|trainer.py:2936] 2024-02-14 20:06:52,097 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-100\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 20:06:52,128 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 20:06:52,129 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-100/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 20:07:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0754, 'learning_rate': 4.9994e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0754, 'learning_rate': 4.999395511092461e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:08:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0866, 'learning_rate': 4.9993e-05, 'epoch': 0.02}\n",
      "{'loss': 0.0866, 'learning_rate': 4.999336572604175e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:08:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.1329, 'learning_rate': 4.9993e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1329, 'learning_rate': 4.9992748932868926e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:09:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1484, 'learning_rate': 4.9992e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1484, 'learning_rate': 4.99921047320825e-05, 'epoch': 0.02}\n",
      "02/14/2024 20:09:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1019, 'learning_rate': 4.9991e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1019, 'learning_rate': 4.999143312438893e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:10:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1700, 'learning_rate': 4.9991e-05, 'epoch': 0.03}\n",
      "{'loss': 0.17, 'learning_rate': 4.999073411052472e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:11:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.1377, 'learning_rate': 4.9990e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1377, 'learning_rate': 4.999000769125641e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:11:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.1014, 'learning_rate': 4.9989e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1014, 'learning_rate': 4.998925386738063e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:12:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.2323, 'learning_rate': 4.9988e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2323, 'learning_rate': 4.998847263972401e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:13:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.1873, 'learning_rate': 4.9988e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1873, 'learning_rate': 4.998766400914329e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:13:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.1347, 'learning_rate': 4.9987e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1347, 'learning_rate': 4.998682797652522e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:14:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1367, 'learning_rate': 4.9986e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1367, 'learning_rate': 4.9985964542786614e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:14:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0791, 'learning_rate': 4.9985e-05, 'epoch': 0.03}\n",
      "{'loss': 0.0791, 'learning_rate': 4.998507370887433e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:15:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.1181, 'learning_rate': 4.9984e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1181, 'learning_rate': 4.998415547576527e-05, 'epoch': 0.03}\n",
      "02/14/2024 20:16:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.1445, 'learning_rate': 4.9983e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1445, 'learning_rate': 4.998320984446641e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:16:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1940, 'learning_rate': 4.9982e-05, 'epoch': 0.04}\n",
      "{'loss': 0.194, 'learning_rate': 4.998223681601473e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:17:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.1085, 'learning_rate': 4.9981e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1085, 'learning_rate': 4.998123639147729e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:18:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1183, 'learning_rate': 4.9980e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1183, 'learning_rate': 4.998020857195117e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:18:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.1152, 'learning_rate': 4.9979e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1152, 'learning_rate': 4.99791533585635e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:19:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0735, 'learning_rate': 4.9978e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0735, 'learning_rate': 4.997807075247146e-05, 'epoch': 0.04}\n",
      "[INFO|trainer.py:2936] 2024-02-14 20:19:19,567 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-200\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 20:19:19,596 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 20:19:19,596 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-200/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 20:19:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0916, 'learning_rate': 4.9977e-05, 'epoch': 0.04}\n",
      "{'loss': 0.0916, 'learning_rate': 4.9976960754862254e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:20:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.1493, 'learning_rate': 4.9976e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1493, 'learning_rate': 4.9975823366953124e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:21:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.1116, 'learning_rate': 4.9975e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1116, 'learning_rate': 4.997465858999136e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:21:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1511, 'learning_rate': 4.9973e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1511, 'learning_rate': 4.9973466425254286e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:22:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.1102, 'learning_rate': 4.9972e-05, 'epoch': 0.04}\n",
      "{'loss': 0.1102, 'learning_rate': 4.9972246874049254e-05, 'epoch': 0.04}\n",
      "02/14/2024 20:23:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0928, 'learning_rate': 4.9971e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0928, 'learning_rate': 4.997099993771365e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:23:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.1167, 'learning_rate': 4.9970e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1167, 'learning_rate': 4.996972561761489e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:24:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.1396, 'learning_rate': 4.9968e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1396, 'learning_rate': 4.996842391515044e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:24:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0776, 'learning_rate': 4.9967e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0776, 'learning_rate': 4.996709483174776e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:25:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.1063, 'learning_rate': 4.9966e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1063, 'learning_rate': 4.996573836886435e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:26:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.1303, 'learning_rate': 4.9964e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1303, 'learning_rate': 4.996435452798774e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:26:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0821, 'learning_rate': 4.9963e-05, 'epoch': 0.05}\n",
      "{'loss': 0.0821, 'learning_rate': 4.99629433106355e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:27:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.1078, 'learning_rate': 4.9962e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1078, 'learning_rate': 4.996150471835518e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:28:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.1266, 'learning_rate': 4.9960e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1266, 'learning_rate': 4.996003875272438e-05, 'epoch': 0.05}\n",
      "02/14/2024 20:28:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0951, 'learning_rate': 4.9959e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0951, 'learning_rate': 4.995854541535071e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:29:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0945, 'learning_rate': 4.9957e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0945, 'learning_rate': 4.9957024707871806e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:29:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0671, 'learning_rate': 4.9955e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0671, 'learning_rate': 4.99554766319553e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:30:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0800, 'learning_rate': 4.9954e-05, 'epoch': 0.06}\n",
      "{'loss': 0.08, 'learning_rate': 4.9953901189298845e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:31:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.1029, 'learning_rate': 4.9952e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1029, 'learning_rate': 4.995229838163012e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:31:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.1732, 'learning_rate': 4.9951e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1732, 'learning_rate': 4.995066821070679e-05, 'epoch': 0.06}\n",
      "[INFO|trainer.py:2936] 2024-02-14 20:31:46,404 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-300\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 20:31:46,433 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 20:31:46,433 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-300/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 20:32:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0911, 'learning_rate': 4.9949e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0911, 'learning_rate': 4.994901067831654e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:33:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.1055, 'learning_rate': 4.9947e-05, 'epoch': 0.06}\n",
      "{'loss': 0.1055, 'learning_rate': 4.994732578627706e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:33:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0731, 'learning_rate': 4.9946e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0731, 'learning_rate': 4.994561353643604e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:34:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0862, 'learning_rate': 4.9944e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0862, 'learning_rate': 4.994387393067117e-05, 'epoch': 0.06}\n",
      "02/14/2024 20:34:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0968, 'learning_rate': 4.9942e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0968, 'learning_rate': 4.994210697089014e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:35:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0915, 'learning_rate': 4.9940e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0915, 'learning_rate': 4.994031265903063e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:36:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0964, 'learning_rate': 4.9938e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0964, 'learning_rate': 4.993849099706035e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:36:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0908, 'learning_rate': 4.9937e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0908, 'learning_rate': 4.993664198697694e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:37:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0974, 'learning_rate': 4.9935e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0974, 'learning_rate': 4.9934765630808095e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:37:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0725, 'learning_rate': 4.9933e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0725, 'learning_rate': 4.9932861930611454e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:38:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1057, 'learning_rate': 4.9931e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1057, 'learning_rate': 4.9930930888474666e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:39:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.1221, 'learning_rate': 4.9929e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1221, 'learning_rate': 4.992897250651535e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:39:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1111, 'learning_rate': 4.9927e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1111, 'learning_rate': 4.992698678688111e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:40:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0837, 'learning_rate': 4.9925e-05, 'epoch': 0.07}\n",
      "{'loss': 0.0837, 'learning_rate': 4.992497373174955e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:41:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1902, 'learning_rate': 4.9923e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1902, 'learning_rate': 4.99229333433282e-05, 'epoch': 0.07}\n",
      "02/14/2024 20:41:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.1628, 'learning_rate': 4.9921e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1628, 'learning_rate': 4.9920865623854615e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:42:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1005, 'learning_rate': 4.9919e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1005, 'learning_rate': 4.9918770575596305e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:42:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1047, 'learning_rate': 4.9917e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1047, 'learning_rate': 4.991664820085074e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:43:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1090, 'learning_rate': 4.9914e-05, 'epoch': 0.08}\n",
      "{'loss': 0.109, 'learning_rate': 4.991449850194538e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:44:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.1302, 'learning_rate': 4.9912e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1302, 'learning_rate': 4.991232148123761e-05, 'epoch': 0.08}\n",
      "[INFO|trainer.py:2936] 2024-02-14 20:44:13,255 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-400\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 20:44:13,284 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 20:44:13,284 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-400/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 20:44:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0911, 'learning_rate': 4.9910e-05, 'epoch': 0.08}\n",
      "{'loss': 0.0911, 'learning_rate': 4.991011714111481e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:45:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.1000, 'learning_rate': 4.9908e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1, 'learning_rate': 4.990788548399432e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:46:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.1014, 'learning_rate': 4.9906e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1014, 'learning_rate': 4.99056265123234e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:46:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0890, 'learning_rate': 4.9903e-05, 'epoch': 0.08}\n",
      "{'loss': 0.089, 'learning_rate': 4.990334022857932e-05, 'epoch': 0.08}\n",
      "02/14/2024 20:47:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.1135, 'learning_rate': 4.9901e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1135, 'learning_rate': 4.990102663526924e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:47:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.1081, 'learning_rate': 4.9899e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1081, 'learning_rate': 4.989868573493032e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:48:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.1245, 'learning_rate': 4.9896e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1245, 'learning_rate': 4.989631753012964e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:49:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.1284, 'learning_rate': 4.9894e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1284, 'learning_rate': 4.9893922023464236e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:49:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.1132, 'learning_rate': 4.9891e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1132, 'learning_rate': 4.989149921756105e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:50:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0906, 'learning_rate': 4.9889e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0906, 'learning_rate': 4.9889049115077005e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:51:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0852, 'learning_rate': 4.9887e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0852, 'learning_rate': 4.988657171869893e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:51:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.0601, 'learning_rate': 4.9884e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0601, 'learning_rate': 4.98840670311436e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:52:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0687, 'learning_rate': 4.9882e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0687, 'learning_rate': 4.988153505515771e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:52:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0902, 'learning_rate': 4.9879e-05, 'epoch': 0.09}\n",
      "{'loss': 0.0902, 'learning_rate': 4.987897579351788e-05, 'epoch': 0.09}\n",
      "02/14/2024 20:53:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0952, 'learning_rate': 4.9876e-05, 'epoch': 0.10}\n",
      "{'loss': 0.0952, 'learning_rate': 4.987638924903067e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:54:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.1140, 'learning_rate': 4.9874e-05, 'epoch': 0.10}\n",
      "{'loss': 0.114, 'learning_rate': 4.987377542453251e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:54:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0977, 'learning_rate': 4.9871e-05, 'epoch': 0.10}\n",
      "{'loss': 0.0977, 'learning_rate': 4.9871134322889804e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:55:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.1037, 'learning_rate': 4.9868e-05, 'epoch': 0.10}\n",
      "{'loss': 0.1037, 'learning_rate': 4.986846594699883e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:56:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.1142, 'learning_rate': 4.9866e-05, 'epoch': 0.10}\n",
      "{'loss': 0.1142, 'learning_rate': 4.986577029978581e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:56:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.0968, 'learning_rate': 4.9863e-05, 'epoch': 0.10}\n",
      "{'loss': 0.0968, 'learning_rate': 4.9863047384206835e-05, 'epoch': 0.1}\n",
      "[INFO|trainer.py:2936] 2024-02-14 20:56:40,552 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-500\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 20:56:40,579 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 20:56:40,579 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 20:57:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0841, 'learning_rate': 4.9860e-05, 'epoch': 0.10}\n",
      "{'loss': 0.0841, 'learning_rate': 4.986029720324792e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:57:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0687, 'learning_rate': 4.9858e-05, 'epoch': 0.10}\n",
      "{'loss': 0.0687, 'learning_rate': 4.9857519759924974e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:58:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.1284, 'learning_rate': 4.9855e-05, 'epoch': 0.10}\n",
      "{'loss': 0.1284, 'learning_rate': 4.9854715057283805e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:59:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0960, 'learning_rate': 4.9852e-05, 'epoch': 0.10}\n",
      "{'loss': 0.096, 'learning_rate': 4.985188309840012e-05, 'epoch': 0.1}\n",
      "02/14/2024 20:59:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.1064, 'learning_rate': 4.9849e-05, 'epoch': 0.10}\n",
      "{'loss': 0.1064, 'learning_rate': 4.98490238863795e-05, 'epoch': 0.1}\n",
      "02/14/2024 21:00:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.1270, 'learning_rate': 4.9846e-05, 'epoch': 0.11}\n",
      "{'loss': 0.127, 'learning_rate': 4.984613742435742e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:01:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.1092, 'learning_rate': 4.9843e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1092, 'learning_rate': 4.984322371549924e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:01:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0736, 'learning_rate': 4.9840e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0736, 'learning_rate': 4.984028276300021e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:02:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.1389, 'learning_rate': 4.9837e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1389, 'learning_rate': 4.983731457008544e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:02:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1198, 'learning_rate': 4.9834e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1198, 'learning_rate': 4.983431914000991e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:03:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.1084, 'learning_rate': 4.9831e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1084, 'learning_rate': 4.9831296476058484e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:04:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1264, 'learning_rate': 4.9828e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1264, 'learning_rate': 4.982824658154589e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:04:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0958, 'learning_rate': 4.9825e-05, 'epoch': 0.11}\n",
      "{'loss': 0.0958, 'learning_rate': 4.982516945981669e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:05:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.1142, 'learning_rate': 4.9822e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1142, 'learning_rate': 4.982206511424534e-05, 'epoch': 0.11}\n",
      "02/14/2024 21:05:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0943, 'learning_rate': 4.9819e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0943, 'learning_rate': 4.981893354823614e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:06:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0775, 'learning_rate': 4.9816e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0775, 'learning_rate': 4.9815774765223226e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:07:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.1059, 'learning_rate': 4.9813e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1059, 'learning_rate': 4.98125887686706e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:07:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.1114, 'learning_rate': 4.9809e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1114, 'learning_rate': 4.980937556207208e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:08:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.1081, 'learning_rate': 4.9806e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1081, 'learning_rate': 4.980613514895136e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:09:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1032, 'learning_rate': 4.9803e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1032, 'learning_rate': 4.980286753286195e-05, 'epoch': 0.12}\n",
      "[INFO|trainer.py:2936] 2024-02-14 21:09:05,526 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-600\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 21:09:05,558 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 21:09:05,559 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-600/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 21:09:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.1030, 'learning_rate': 4.9800e-05, 'epoch': 0.12}\n",
      "{'loss': 0.103, 'learning_rate': 4.9799572717387175e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:10:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0664, 'learning_rate': 4.9796e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0664, 'learning_rate': 4.9796250706140224e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:10:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0656, 'learning_rate': 4.9793e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0656, 'learning_rate': 4.9792901502764075e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:11:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0761, 'learning_rate': 4.9790e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0761, 'learning_rate': 4.9789525110931545e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:12:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0658, 'learning_rate': 4.9786e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0658, 'learning_rate': 4.9786121534345265e-05, 'epoch': 0.12}\n",
      "02/14/2024 21:12:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1397, 'learning_rate': 4.9783e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1397, 'learning_rate': 4.978269077673767e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:13:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0826, 'learning_rate': 4.9779e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0826, 'learning_rate': 4.977923284187101e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:14:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.1090, 'learning_rate': 4.9776e-05, 'epoch': 0.13}\n",
      "{'loss': 0.109, 'learning_rate': 4.977574773353732e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:14:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1103, 'learning_rate': 4.9772e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1103, 'learning_rate': 4.977223545555847e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:15:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0710, 'learning_rate': 4.9769e-05, 'epoch': 0.13}\n",
      "{'loss': 0.071, 'learning_rate': 4.976869601178609e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:15:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.1253, 'learning_rate': 4.9765e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1253, 'learning_rate': 4.976512940610163e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:16:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.1171, 'learning_rate': 4.9762e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1171, 'learning_rate': 4.976153564241628e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:17:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0772, 'learning_rate': 4.9758e-05, 'epoch': 0.13}\n",
      "{'loss': 0.0772, 'learning_rate': 4.9757914724671074e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:17:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.1214, 'learning_rate': 4.9754e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1214, 'learning_rate': 4.975426665683678e-05, 'epoch': 0.13}\n",
      "02/14/2024 21:18:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.1290, 'learning_rate': 4.9751e-05, 'epoch': 0.14}\n",
      "{'loss': 0.129, 'learning_rate': 4.975059144291394e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:19:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0940, 'learning_rate': 4.9747e-05, 'epoch': 0.14}\n",
      "{'loss': 0.094, 'learning_rate': 4.9746889086932895e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:19:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1146, 'learning_rate': 4.9743e-05, 'epoch': 0.14}\n",
      "{'loss': 0.1146, 'learning_rate': 4.974315959295373e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:20:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0867, 'learning_rate': 4.9739e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0867, 'learning_rate': 4.9739402965066276e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:20:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.1726, 'learning_rate': 4.9736e-05, 'epoch': 0.14}\n",
      "{'loss': 0.1726, 'learning_rate': 4.973561920739015e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:21:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.1040, 'learning_rate': 4.9732e-05, 'epoch': 0.14}\n",
      "{'loss': 0.104, 'learning_rate': 4.9731808324074717e-05, 'epoch': 0.14}\n",
      "[INFO|trainer.py:2936] 2024-02-14 21:21:32,524 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-700\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 21:21:32,554 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 21:21:32,554 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-700/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 21:22:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0789, 'learning_rate': 4.9728e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0789, 'learning_rate': 4.9727970319299044e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:22:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0864, 'learning_rate': 4.9724e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0864, 'learning_rate': 4.972410519727201e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:23:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0828, 'learning_rate': 4.9720e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0828, 'learning_rate': 4.972021296223217e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:24:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0985, 'learning_rate': 4.9716e-05, 'epoch': 0.14}\n",
      "{'loss': 0.0985, 'learning_rate': 4.971629361844785e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:24:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1695, 'learning_rate': 4.9712e-05, 'epoch': 0.14}\n",
      "{'loss': 0.1695, 'learning_rate': 4.971234717021709e-05, 'epoch': 0.14}\n",
      "02/14/2024 21:25:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0859, 'learning_rate': 4.9708e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0859, 'learning_rate': 4.9708373621867656e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:25:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0802, 'learning_rate': 4.9704e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0802, 'learning_rate': 4.970437297775702e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:26:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.1064, 'learning_rate': 4.9700e-05, 'epoch': 0.15}\n",
      "{'loss': 0.1064, 'learning_rate': 4.970034524227238e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:27:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0582, 'learning_rate': 4.9696e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0582, 'learning_rate': 4.9696290419830654e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:27:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.1381, 'learning_rate': 4.9692e-05, 'epoch': 0.15}\n",
      "{'loss': 0.1381, 'learning_rate': 4.9692208514878444e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:28:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0773, 'learning_rate': 4.9688e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0773, 'learning_rate': 4.968809953189206e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:29:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0846, 'learning_rate': 4.9684e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0846, 'learning_rate': 4.968396347537751e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:29:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0686, 'learning_rate': 4.9680e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0686, 'learning_rate': 4.967980034987048e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:30:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0806, 'learning_rate': 4.9676e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0806, 'learning_rate': 4.967561015993635e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:30:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0939, 'learning_rate': 4.9671e-05, 'epoch': 0.15}\n",
      "{'loss': 0.0939, 'learning_rate': 4.9671392910170185e-05, 'epoch': 0.15}\n",
      "02/14/2024 21:31:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.1396, 'learning_rate': 4.9667e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1396, 'learning_rate': 4.96671486051967e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:32:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0922, 'learning_rate': 4.9663e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0922, 'learning_rate': 4.966287724967031e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:32:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.1067, 'learning_rate': 4.9659e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1067, 'learning_rate': 4.9658578848275076e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:33:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.1019, 'learning_rate': 4.9654e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1019, 'learning_rate': 4.9654253405724724e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:34:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0694, 'learning_rate': 4.9650e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0694, 'learning_rate': 4.964990092676263e-05, 'epoch': 0.16}\n",
      "[INFO|trainer.py:2936] 2024-02-14 21:34:00,891 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-800\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 21:34:00,920 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 21:34:00,920 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-800/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 21:34:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0581, 'learning_rate': 4.9646e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0581, 'learning_rate': 4.964552141616181e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:35:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1339, 'learning_rate': 4.9641e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1339, 'learning_rate': 4.9641114878724956e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:35:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0968, 'learning_rate': 4.9637e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0968, 'learning_rate': 4.963668131928436e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:36:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0986, 'learning_rate': 4.9632e-05, 'epoch': 0.16}\n",
      "{'loss': 0.0986, 'learning_rate': 4.9632220742701965e-05, 'epoch': 0.16}\n",
      "02/14/2024 21:37:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0731, 'learning_rate': 4.9628e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0731, 'learning_rate': 4.962773315386935e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:37:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.1177, 'learning_rate': 4.9623e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1177, 'learning_rate': 4.9623218557707694e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:38:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0563, 'learning_rate': 4.9619e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0563, 'learning_rate': 4.961867695916782e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:38:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0578, 'learning_rate': 4.9614e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0578, 'learning_rate': 4.9614108363230135e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:39:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0977, 'learning_rate': 4.9610e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0977, 'learning_rate': 4.960951277490467e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:40:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0746, 'learning_rate': 4.9605e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0746, 'learning_rate': 4.960489019923105e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:40:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0918, 'learning_rate': 4.9600e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0918, 'learning_rate': 4.9600240641278496e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:41:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.1348, 'learning_rate': 4.9596e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1348, 'learning_rate': 4.959556410614582e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:42:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0699, 'learning_rate': 4.9591e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0699, 'learning_rate': 4.959086059896141e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:42:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0715, 'learning_rate': 4.9586e-05, 'epoch': 0.17}\n",
      "{'loss': 0.0715, 'learning_rate': 4.958613012488324e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:43:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1135, 'learning_rate': 4.9581e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1135, 'learning_rate': 4.958137268909887e-05, 'epoch': 0.17}\n",
      "02/14/2024 21:43:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1055, 'learning_rate': 4.9577e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1055, 'learning_rate': 4.9576588296825386e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:44:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0916, 'learning_rate': 4.9572e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0916, 'learning_rate': 4.957177695330948e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:45:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0658, 'learning_rate': 4.9567e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0658, 'learning_rate': 4.9566938663827377e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:45:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0747, 'learning_rate': 4.9562e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0747, 'learning_rate': 4.956207343368485e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:46:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0809, 'learning_rate': 4.9557e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0809, 'learning_rate': 4.9557181268217227e-05, 'epoch': 0.18}\n",
      "[INFO|trainer.py:2936] 2024-02-14 21:46:28,335 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-900\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 21:46:28,364 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 21:46:28,364 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-900/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 21:47:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0773, 'learning_rate': 4.9552e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0773, 'learning_rate': 4.955226217278935e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:47:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0960, 'learning_rate': 4.9547e-05, 'epoch': 0.18}\n",
      "{'loss': 0.096, 'learning_rate': 4.954731615279563e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:48:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0951, 'learning_rate': 4.9542e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0951, 'learning_rate': 4.9542343213659974e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:48:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1266, 'learning_rate': 4.9537e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1266, 'learning_rate': 4.953734336083583e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:49:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0909, 'learning_rate': 4.9532e-05, 'epoch': 0.18}\n",
      "{'loss': 0.0909, 'learning_rate': 4.9532316599806124e-05, 'epoch': 0.18}\n",
      "02/14/2024 21:50:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0605, 'learning_rate': 4.9527e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0605, 'learning_rate': 4.952726293608335e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:50:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0799, 'learning_rate': 4.9522e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0799, 'learning_rate': 4.9522182375209455e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:51:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0660, 'learning_rate': 4.9517e-05, 'epoch': 0.19}\n",
      "{'loss': 0.066, 'learning_rate': 4.951707492275589e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:52:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1010, 'learning_rate': 4.9512e-05, 'epoch': 0.19}\n",
      "{'loss': 0.101, 'learning_rate': 4.951194058432361e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:52:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0700, 'learning_rate': 4.9507e-05, 'epoch': 0.19}\n",
      "{'loss': 0.07, 'learning_rate': 4.9506779365543046e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:53:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0740, 'learning_rate': 4.9502e-05, 'epoch': 0.19}\n",
      "{'loss': 0.074, 'learning_rate': 4.950159127207411e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:53:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.1070, 'learning_rate': 4.9496e-05, 'epoch': 0.19}\n",
      "{'loss': 0.107, 'learning_rate': 4.949637630960617e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:54:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0716, 'learning_rate': 4.9491e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0716, 'learning_rate': 4.9491134483858095e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:55:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0937, 'learning_rate': 4.9486e-05, 'epoch': 0.19}\n",
      "{'loss': 0.0937, 'learning_rate': 4.948586580057816e-05, 'epoch': 0.19}\n",
      "02/14/2024 21:55:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.1107, 'learning_rate': 4.9481e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1107, 'learning_rate': 4.9480570265544144e-05, 'epoch': 0.2}\n",
      "02/14/2024 21:56:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0981, 'learning_rate': 4.9475e-05, 'epoch': 0.20}\n",
      "{'loss': 0.0981, 'learning_rate': 4.947524788456325e-05, 'epoch': 0.2}\n",
      "02/14/2024 21:57:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.1366, 'learning_rate': 4.9470e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1366, 'learning_rate': 4.9469898663472105e-05, 'epoch': 0.2}\n",
      "02/14/2024 21:57:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.1074, 'learning_rate': 4.9465e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1074, 'learning_rate': 4.9464522608136805e-05, 'epoch': 0.2}\n",
      "02/14/2024 21:58:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0984, 'learning_rate': 4.9459e-05, 'epoch': 0.20}\n",
      "{'loss': 0.0984, 'learning_rate': 4.945911972445284e-05, 'epoch': 0.2}\n",
      "02/14/2024 21:58:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0740, 'learning_rate': 4.9454e-05, 'epoch': 0.20}\n",
      "{'loss': 0.074, 'learning_rate': 4.9453690018345144e-05, 'epoch': 0.2}\n",
      "[INFO|trainer.py:2936] 2024-02-14 21:58:53,938 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1000\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 21:58:53,966 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 21:58:53,966 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 21:59:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.1111, 'learning_rate': 4.9448e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1111, 'learning_rate': 4.944823349576805e-05, 'epoch': 0.2}\n",
      "02/14/2024 22:00:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0878, 'learning_rate': 4.9443e-05, 'epoch': 0.20}\n",
      "{'loss': 0.0878, 'learning_rate': 4.9442750162705295e-05, 'epoch': 0.2}\n",
      "02/14/2024 22:00:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0833, 'learning_rate': 4.9437e-05, 'epoch': 0.20}\n",
      "{'loss': 0.0833, 'learning_rate': 4.943724002517005e-05, 'epoch': 0.2}\n",
      "02/14/2024 22:01:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.1179, 'learning_rate': 4.9432e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1179, 'learning_rate': 4.943170308920484e-05, 'epoch': 0.2}\n",
      "02/14/2024 22:02:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.1207, 'learning_rate': 4.9426e-05, 'epoch': 0.20}\n",
      "{'loss': 0.1207, 'learning_rate': 4.94261393608816e-05, 'epoch': 0.2}\n",
      "02/14/2024 22:02:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.1033, 'learning_rate': 4.9421e-05, 'epoch': 0.21}\n",
      "{'loss': 0.1033, 'learning_rate': 4.942054884630162e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:03:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0532, 'learning_rate': 4.9415e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0532, 'learning_rate': 4.941493155159562e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:03:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0822, 'learning_rate': 4.9409e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0822, 'learning_rate': 4.940928748292363e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:04:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0855, 'learning_rate': 4.9404e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0855, 'learning_rate': 4.940361664647506e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:05:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0722, 'learning_rate': 4.9398e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0722, 'learning_rate': 4.939791904846869e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:05:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.0920, 'learning_rate': 4.9392e-05, 'epoch': 0.21}\n",
      "{'loss': 0.092, 'learning_rate': 4.939219469515263e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:06:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0876, 'learning_rate': 4.9386e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0876, 'learning_rate': 4.938644359280433e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:07:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0510, 'learning_rate': 4.9381e-05, 'epoch': 0.21}\n",
      "{'loss': 0.051, 'learning_rate': 4.938066574773059e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:07:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0921, 'learning_rate': 4.9375e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0921, 'learning_rate': 4.937486116626752e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:08:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0976, 'learning_rate': 4.9369e-05, 'epoch': 0.21}\n",
      "{'loss': 0.0976, 'learning_rate': 4.936902985478055e-05, 'epoch': 0.21}\n",
      "02/14/2024 22:08:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1056, 'learning_rate': 4.9363e-05, 'epoch': 0.22}\n",
      "{'loss': 0.1056, 'learning_rate': 4.9363171819664434e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:09:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0574, 'learning_rate': 4.9357e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0574, 'learning_rate': 4.935728706734322e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:10:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0996, 'learning_rate': 4.9351e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0996, 'learning_rate': 4.935137560427027e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:10:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0589, 'learning_rate': 4.9345e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0589, 'learning_rate': 4.934543743692822e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:11:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0639, 'learning_rate': 4.9339e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0639, 'learning_rate': 4.933947257182901e-05, 'epoch': 0.22}\n",
      "[INFO|trainer.py:2936] 2024-02-14 22:11:22,086 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1100\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 22:11:22,115 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 22:11:22,115 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1100/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 22:11:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0917, 'learning_rate': 4.9333e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0917, 'learning_rate': 4.933348101551383e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:12:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0740, 'learning_rate': 4.9327e-05, 'epoch': 0.22}\n",
      "{'loss': 0.074, 'learning_rate': 4.9327462774553166e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:13:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0612, 'learning_rate': 4.9321e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0612, 'learning_rate': 4.932141785554676e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:13:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0779, 'learning_rate': 4.9315e-05, 'epoch': 0.22}\n",
      "{'loss': 0.0779, 'learning_rate': 4.9315346265123594e-05, 'epoch': 0.22}\n",
      "02/14/2024 22:14:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0748, 'learning_rate': 4.9309e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0748, 'learning_rate': 4.9309248009941914e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:15:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0861, 'learning_rate': 4.9303e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0861, 'learning_rate': 4.930312309668922e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:15:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0989, 'learning_rate': 4.9297e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0989, 'learning_rate': 4.929697153208222e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:16:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0955, 'learning_rate': 4.9291e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0955, 'learning_rate': 4.929079332286685e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:16:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.1570, 'learning_rate': 4.9285e-05, 'epoch': 0.23}\n",
      "{'loss': 0.157, 'learning_rate': 4.928458847581829e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:17:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0781, 'learning_rate': 4.9278e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0781, 'learning_rate': 4.9278356997740904e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:18:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0794, 'learning_rate': 4.9272e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0794, 'learning_rate': 4.9272098895468277e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:18:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1065, 'learning_rate': 4.9266e-05, 'epoch': 0.23}\n",
      "{'loss': 0.1065, 'learning_rate': 4.9265814175863186e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:19:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0819, 'learning_rate': 4.9260e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0819, 'learning_rate': 4.9259502845817594e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:20:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0656, 'learning_rate': 4.9253e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0656, 'learning_rate': 4.925316491225265e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:20:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0589, 'learning_rate': 4.9247e-05, 'epoch': 0.23}\n",
      "{'loss': 0.0589, 'learning_rate': 4.924680038211867e-05, 'epoch': 0.23}\n",
      "02/14/2024 22:21:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0880, 'learning_rate': 4.9240e-05, 'epoch': 0.24}\n",
      "{'loss': 0.088, 'learning_rate': 4.924040926239515e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:21:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.1225, 'learning_rate': 4.9234e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1225, 'learning_rate': 4.923399156009073e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:22:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0758, 'learning_rate': 4.9228e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0758, 'learning_rate': 4.9227547282243214e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:23:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0756, 'learning_rate': 4.9221e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0756, 'learning_rate': 4.9221076435919546e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:23:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0849, 'learning_rate': 4.9215e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0849, 'learning_rate': 4.9214579028215776e-05, 'epoch': 0.24}\n",
      "[INFO|trainer.py:2936] 2024-02-14 22:23:49,996 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1200\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 22:23:50,024 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 22:23:50,025 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1200/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 22:24:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0990, 'learning_rate': 4.9208e-05, 'epoch': 0.24}\n",
      "{'loss': 0.099, 'learning_rate': 4.9208055066257144e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:25:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0784, 'learning_rate': 4.9202e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0784, 'learning_rate': 4.9201504557197955e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:25:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.1020, 'learning_rate': 4.9195e-05, 'epoch': 0.24}\n",
      "{'loss': 0.102, 'learning_rate': 4.919492750822164e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:26:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0923, 'learning_rate': 4.9188e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0923, 'learning_rate': 4.9188323926540746e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:26:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0878, 'learning_rate': 4.9182e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0878, 'learning_rate': 4.918169381939692e-05, 'epoch': 0.24}\n",
      "02/14/2024 22:27:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0804, 'learning_rate': 4.9175e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0804, 'learning_rate': 4.917503719406088e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:28:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0954, 'learning_rate': 4.9168e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0954, 'learning_rate': 4.9168354057832424e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:28:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0786, 'learning_rate': 4.9162e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0786, 'learning_rate': 4.916164441804044e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:29:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0714, 'learning_rate': 4.9155e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0714, 'learning_rate': 4.915490828204287e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:30:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0886, 'learning_rate': 4.9148e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0886, 'learning_rate': 4.914814565722671e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:30:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0911, 'learning_rate': 4.9141e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0911, 'learning_rate': 4.914135655100801e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:31:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0648, 'learning_rate': 4.9135e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0648, 'learning_rate': 4.913454097083185e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:31:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0685, 'learning_rate': 4.9128e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0685, 'learning_rate': 4.912769892417236e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:32:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0767, 'learning_rate': 4.9121e-05, 'epoch': 0.25}\n",
      "{'loss': 0.0767, 'learning_rate': 4.912083041853267e-05, 'epoch': 0.25}\n",
      "02/14/2024 22:33:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0759, 'learning_rate': 4.9114e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0759, 'learning_rate': 4.9113935461444955e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:33:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0772, 'learning_rate': 4.9107e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0772, 'learning_rate': 4.910701406047037e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:34:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0551, 'learning_rate': 4.9100e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0551, 'learning_rate': 4.910006622319908e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:35:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.1029, 'learning_rate': 4.9093e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1029, 'learning_rate': 4.909309195725025e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:35:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.0809, 'learning_rate': 4.9086e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0809, 'learning_rate': 4.9086091270272e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:36:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0608, 'learning_rate': 4.9079e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0608, 'learning_rate': 4.907906416994146e-05, 'epoch': 0.26}\n",
      "[INFO|trainer.py:2936] 2024-02-14 22:36:18,505 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1300\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 22:36:18,533 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 22:36:18,533 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1300/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 22:36:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0799, 'learning_rate': 4.9072e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0799, 'learning_rate': 4.907201066396469e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:37:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.1139, 'learning_rate': 4.9065e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1139, 'learning_rate': 4.906493076007674e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:38:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.1049, 'learning_rate': 4.9058e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1049, 'learning_rate': 4.905782446604159e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:38:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0762, 'learning_rate': 4.9051e-05, 'epoch': 0.26}\n",
      "{'loss': 0.0762, 'learning_rate': 4.905069178965215e-05, 'epoch': 0.26}\n",
      "02/14/2024 22:39:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0695, 'learning_rate': 4.9044e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0695, 'learning_rate': 4.9043532738730284e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:40:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0976, 'learning_rate': 4.9036e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0976, 'learning_rate': 4.903634732112678e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:40:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1072, 'learning_rate': 4.9029e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1072, 'learning_rate': 4.90291355447213e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:41:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.1109, 'learning_rate': 4.9022e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1109, 'learning_rate': 4.902189741742247e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:41:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.1104, 'learning_rate': 4.9015e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1104, 'learning_rate': 4.9014632947167763e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:42:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.1082, 'learning_rate': 4.9007e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1082, 'learning_rate': 4.900734214192358e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:43:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0771, 'learning_rate': 4.9000e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0771, 'learning_rate': 4.900002500968517e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:43:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0845, 'learning_rate': 4.8993e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0845, 'learning_rate': 4.899268155847667e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:44:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0778, 'learning_rate': 4.8985e-05, 'epoch': 0.27}\n",
      "{'loss': 0.0778, 'learning_rate': 4.898531179635107e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:45:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.1103, 'learning_rate': 4.8978e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1103, 'learning_rate': 4.897791573139023e-05, 'epoch': 0.27}\n",
      "02/14/2024 22:45:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0784, 'learning_rate': 4.8970e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0784, 'learning_rate': 4.8970493371704826e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:46:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0831, 'learning_rate': 4.8963e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0831, 'learning_rate': 4.89630447254344e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:46:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0971, 'learning_rate': 4.8956e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0971, 'learning_rate': 4.895556980074729e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:47:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.1119, 'learning_rate': 4.8948e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1119, 'learning_rate': 4.8948068605840694e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:48:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1145, 'learning_rate': 4.8941e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1145, 'learning_rate': 4.8940541148940555e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:48:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0895, 'learning_rate': 4.8933e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0895, 'learning_rate': 4.893298743830168e-05, 'epoch': 0.28}\n",
      "[INFO|trainer.py:2936] 2024-02-14 22:48:44,409 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1400\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 22:48:44,437 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 22:48:44,437 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1400/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 22:49:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0742, 'learning_rate': 4.8925e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0742, 'learning_rate': 4.8925407482207634e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:49:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.1195, 'learning_rate': 4.8918e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1195, 'learning_rate': 4.891780128897077e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:50:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1256, 'learning_rate': 4.8910e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1256, 'learning_rate': 4.8910168866932195e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:51:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0596, 'learning_rate': 4.8903e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0596, 'learning_rate': 4.890251022446181e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:51:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0982, 'learning_rate': 4.8895e-05, 'epoch': 0.28}\n",
      "{'loss': 0.0982, 'learning_rate': 4.8894825369958255e-05, 'epoch': 0.28}\n",
      "02/14/2024 22:52:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0979, 'learning_rate': 4.8887e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0979, 'learning_rate': 4.8887114311848915e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:53:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1058, 'learning_rate': 4.8879e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1058, 'learning_rate': 4.88793770585899e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:53:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0806, 'learning_rate': 4.8872e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0806, 'learning_rate': 4.887161361866608e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:54:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0706, 'learning_rate': 4.8864e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0706, 'learning_rate': 4.8863824000590995e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:54:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.0800, 'learning_rate': 4.8856e-05, 'epoch': 0.29}\n",
      "{'loss': 0.08, 'learning_rate': 4.8856008212906925e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:55:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0514, 'learning_rate': 4.8848e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0514, 'learning_rate': 4.8848166264184844e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:56:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0801, 'learning_rate': 4.8840e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0801, 'learning_rate': 4.88402981630244e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:56:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1126, 'learning_rate': 4.8832e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1126, 'learning_rate': 4.883240391805394e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:57:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0777, 'learning_rate': 4.8824e-05, 'epoch': 0.29}\n",
      "{'loss': 0.0777, 'learning_rate': 4.882448353793048e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:58:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1254, 'learning_rate': 4.8817e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1254, 'learning_rate': 4.881653703133966e-05, 'epoch': 0.29}\n",
      "02/14/2024 22:58:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0815, 'learning_rate': 4.8809e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0815, 'learning_rate': 4.880856440699582e-05, 'epoch': 0.3}\n",
      "02/14/2024 22:59:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0963, 'learning_rate': 4.8801e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0963, 'learning_rate': 4.880056567364192e-05, 'epoch': 0.3}\n",
      "02/14/2024 22:59:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0849, 'learning_rate': 4.8793e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0849, 'learning_rate': 4.879254084004955e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:00:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0832, 'learning_rate': 4.8784e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0832, 'learning_rate': 4.8784489915018905e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:01:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0935, 'learning_rate': 4.8776e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0935, 'learning_rate': 4.877641290737884e-05, 'epoch': 0.3}\n",
      "[INFO|trainer.py:2936] 2024-02-14 23:01:11,246 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1500\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 23:01:11,276 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 23:01:11,276 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 23:01:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.1262, 'learning_rate': 4.8768e-05, 'epoch': 0.30}\n",
      "{'loss': 0.1262, 'learning_rate': 4.876830982598677e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:02:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0700, 'learning_rate': 4.8760e-05, 'epoch': 0.30}\n",
      "{'loss': 0.07, 'learning_rate': 4.876018067972872e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:03:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0405, 'learning_rate': 4.8752e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0405, 'learning_rate': 4.8752025477519295e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:03:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0721, 'learning_rate': 4.8744e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0721, 'learning_rate': 4.874384422830167e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:04:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0863, 'learning_rate': 4.8736e-05, 'epoch': 0.30}\n",
      "{'loss': 0.0863, 'learning_rate': 4.87356369410476e-05, 'epoch': 0.3}\n",
      "02/14/2024 23:04:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0946, 'learning_rate': 4.8727e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0946, 'learning_rate': 4.8727403624757365e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:05:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0705, 'learning_rate': 4.8719e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0705, 'learning_rate': 4.8719144288459814e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:06:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0661, 'learning_rate': 4.8711e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0661, 'learning_rate': 4.871085894121233e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:06:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0954, 'learning_rate': 4.8703e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0954, 'learning_rate': 4.87025475921008e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:07:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0697, 'learning_rate': 4.8694e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0697, 'learning_rate': 4.869421025023965e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:08:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0786, 'learning_rate': 4.8686e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0786, 'learning_rate': 4.8685846924771774e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:08:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0979, 'learning_rate': 4.8677e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0979, 'learning_rate': 4.867745762486861e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:09:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0800, 'learning_rate': 4.8669e-05, 'epoch': 0.31}\n",
      "{'loss': 0.08, 'learning_rate': 4.866904235973005e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:09:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0942, 'learning_rate': 4.8661e-05, 'epoch': 0.31}\n",
      "{'loss': 0.0942, 'learning_rate': 4.866060113858444e-05, 'epoch': 0.31}\n",
      "02/14/2024 23:10:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0801, 'learning_rate': 4.8652e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0801, 'learning_rate': 4.8652133970688636e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:11:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0495, 'learning_rate': 4.8644e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0495, 'learning_rate': 4.864364086532792e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:11:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.0809, 'learning_rate': 4.8635e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0809, 'learning_rate': 4.863512183181603e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:12:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0633, 'learning_rate': 4.8627e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0633, 'learning_rate': 4.862657687949512e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:13:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0673, 'learning_rate': 4.8618e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0673, 'learning_rate': 4.861800601773579e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:13:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0962, 'learning_rate': 4.8609e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0962, 'learning_rate': 4.860940925593703e-05, 'epoch': 0.32}\n",
      "[INFO|trainer.py:2936] 2024-02-14 23:13:37,725 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1600\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 23:13:37,754 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 23:13:37,754 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1600/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 23:14:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1371, 'learning_rate': 4.8601e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1371, 'learning_rate': 4.860078660352625e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:14:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0618, 'learning_rate': 4.8592e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0618, 'learning_rate': 4.859213806995924e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:15:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.1041, 'learning_rate': 4.8583e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1041, 'learning_rate': 4.8583463664720176e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:16:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0769, 'learning_rate': 4.8575e-05, 'epoch': 0.32}\n",
      "{'loss': 0.0769, 'learning_rate': 4.8574763397321614e-05, 'epoch': 0.32}\n",
      "02/14/2024 23:16:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0612, 'learning_rate': 4.8566e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0612, 'learning_rate': 4.856603727730447e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:17:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0486, 'learning_rate': 4.8557e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0486, 'learning_rate': 4.855728531423798e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:17:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.1001, 'learning_rate': 4.8549e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1001, 'learning_rate': 4.854850751771977e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:18:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0792, 'learning_rate': 4.8540e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0792, 'learning_rate': 4.8539703897375755e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:19:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.1163, 'learning_rate': 4.8531e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1163, 'learning_rate': 4.8530874462860194e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:19:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1264, 'learning_rate': 4.8522e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1264, 'learning_rate': 4.852201922385564e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:20:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.1184, 'learning_rate': 4.8513e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1184, 'learning_rate': 4.851313819007295e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:21:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0852, 'learning_rate': 4.8504e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0852, 'learning_rate': 4.8504231371251255e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:21:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0618, 'learning_rate': 4.8495e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0618, 'learning_rate': 4.849529877715799e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:22:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0915, 'learning_rate': 4.8486e-05, 'epoch': 0.33}\n",
      "{'loss': 0.0915, 'learning_rate': 4.8486340417588835e-05, 'epoch': 0.33}\n",
      "02/14/2024 23:22:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0824, 'learning_rate': 4.8477e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0824, 'learning_rate': 4.847735630236773e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:23:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0615, 'learning_rate': 4.8468e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0615, 'learning_rate': 4.846834644134686e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:24:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.1098, 'learning_rate': 4.8459e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1098, 'learning_rate': 4.845931084440662e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:24:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1138, 'learning_rate': 4.8450e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1138, 'learning_rate': 4.8450249521455695e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:25:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0731, 'learning_rate': 4.8441e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0731, 'learning_rate': 4.844116248243089e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:26:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0879, 'learning_rate': 4.8432e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0879, 'learning_rate': 4.843204973729729e-05, 'epoch': 0.34}\n",
      "[INFO|trainer.py:2936] 2024-02-14 23:26:05,412 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1700\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 23:26:05,441 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 23:26:05,441 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1700/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 23:26:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.1258, 'learning_rate': 4.8423e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1258, 'learning_rate': 4.842291129604812e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:27:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0767, 'learning_rate': 4.8414e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0767, 'learning_rate': 4.841374716870481e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:27:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.0931, 'learning_rate': 4.8405e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0931, 'learning_rate': 4.840455736531695e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:28:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.1206, 'learning_rate': 4.8395e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1206, 'learning_rate': 4.839534189596228e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:29:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0963, 'learning_rate': 4.8386e-05, 'epoch': 0.34}\n",
      "{'loss': 0.0963, 'learning_rate': 4.838610077074669e-05, 'epoch': 0.34}\n",
      "02/14/2024 23:29:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0849, 'learning_rate': 4.8377e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0849, 'learning_rate': 4.837683399980421e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:30:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0724, 'learning_rate': 4.8368e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0724, 'learning_rate': 4.8367541593297e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:31:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1689, 'learning_rate': 4.8358e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1689, 'learning_rate': 4.8358223561415304e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:31:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.1295, 'learning_rate': 4.8349e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1295, 'learning_rate': 4.8348879914377504e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:32:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0892, 'learning_rate': 4.8340e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0892, 'learning_rate': 4.8339510662430046e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:32:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0463, 'learning_rate': 4.8330e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0463, 'learning_rate': 4.8330115815847465e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:33:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.1102, 'learning_rate': 4.8321e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1102, 'learning_rate': 4.832069538493237e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:34:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0931, 'learning_rate': 4.8311e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0931, 'learning_rate': 4.83112493800154e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:34:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1220, 'learning_rate': 4.8302e-05, 'epoch': 0.35}\n",
      "{'loss': 0.122, 'learning_rate': 4.8301777811455276e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:35:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0629, 'learning_rate': 4.8292e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0629, 'learning_rate': 4.8292280689638725e-05, 'epoch': 0.35}\n",
      "02/14/2024 23:36:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0729, 'learning_rate': 4.8283e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0729, 'learning_rate': 4.828275802498051e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:36:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0821, 'learning_rate': 4.8273e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0821, 'learning_rate': 4.827320982792339e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:37:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.1113, 'learning_rate': 4.8264e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1113, 'learning_rate': 4.8263636108938156e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:37:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.1064, 'learning_rate': 4.8254e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1064, 'learning_rate': 4.825403687852354e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:38:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0811, 'learning_rate': 4.8244e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0811, 'learning_rate': 4.8244412147206284e-05, 'epoch': 0.36}\n",
      "[INFO|trainer.py:2936] 2024-02-14 23:38:34,390 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1800\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 23:38:34,419 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 23:38:34,419 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1800/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 23:39:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0829, 'learning_rate': 4.8235e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0829, 'learning_rate': 4.823476192554109e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:39:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0688, 'learning_rate': 4.8225e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0688, 'learning_rate': 4.8225086224110615e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:40:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0819, 'learning_rate': 4.8215e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0819, 'learning_rate': 4.821538505352543e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:41:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0678, 'learning_rate': 4.8206e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0678, 'learning_rate': 4.820565842442408e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:41:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.0889, 'learning_rate': 4.8196e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0889, 'learning_rate': 4.8195906347473e-05, 'epoch': 0.36}\n",
      "02/14/2024 23:42:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0699, 'learning_rate': 4.8186e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0699, 'learning_rate': 4.818612883336654e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:42:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0865, 'learning_rate': 4.8176e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0865, 'learning_rate': 4.8176325892826926e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:43:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.0764, 'learning_rate': 4.8166e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0764, 'learning_rate': 4.81664975366043e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:44:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.1154, 'learning_rate': 4.8157e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1154, 'learning_rate': 4.8156643775476664e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:44:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0721, 'learning_rate': 4.8147e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0721, 'learning_rate': 4.814676462024988e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:45:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0539, 'learning_rate': 4.8137e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0539, 'learning_rate': 4.813686008175762e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:46:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0910, 'learning_rate': 4.8127e-05, 'epoch': 0.37}\n",
      "{'loss': 0.091, 'learning_rate': 4.812693017086145e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:46:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0869, 'learning_rate': 4.8117e-05, 'epoch': 0.37}\n",
      "{'loss': 0.0869, 'learning_rate': 4.8116974898450736e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:47:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.1006, 'learning_rate': 4.8107e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1006, 'learning_rate': 4.810699427544265e-05, 'epoch': 0.37}\n",
      "02/14/2024 23:47:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.1188, 'learning_rate': 4.8097e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1188, 'learning_rate': 4.8096988312782174e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:48:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.1001, 'learning_rate': 4.8087e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1001, 'learning_rate': 4.808695702144206e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:49:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0906, 'learning_rate': 4.8077e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0906, 'learning_rate': 4.8076900412422856e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:49:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0668, 'learning_rate': 4.8067e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0668, 'learning_rate': 4.8066818496752875e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:50:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.1078, 'learning_rate': 4.8057e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1078, 'learning_rate': 4.805671128548816e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:51:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0597, 'learning_rate': 4.8047e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0597, 'learning_rate': 4.8046578789712515e-05, 'epoch': 0.38}\n",
      "[INFO|trainer.py:2936] 2024-02-14 23:51:01,478 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-1900\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-14 23:51:01,507 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-14 23:51:01,507 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-1900/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/14/2024 23:51:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0779, 'learning_rate': 4.8036e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0779, 'learning_rate': 4.803642102053746e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:52:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0831, 'learning_rate': 4.8026e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0831, 'learning_rate': 4.802623798910224e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:52:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0865, 'learning_rate': 4.8016e-05, 'epoch': 0.38}\n",
      "{'loss': 0.0865, 'learning_rate': 4.801602970657379e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:53:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.1371, 'learning_rate': 4.8006e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1371, 'learning_rate': 4.800579618414676e-05, 'epoch': 0.38}\n",
      "02/14/2024 23:54:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0923, 'learning_rate': 4.7996e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0923, 'learning_rate': 4.7995537433043446e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:54:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0951, 'learning_rate': 4.7985e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0951, 'learning_rate': 4.7985253464513825e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:55:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0922, 'learning_rate': 4.7975e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0922, 'learning_rate': 4.797494428983553e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:56:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0874, 'learning_rate': 4.7965e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0874, 'learning_rate': 4.796460992031385e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:56:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0912, 'learning_rate': 4.7954e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0912, 'learning_rate': 4.795425036728168e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:57:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0784, 'learning_rate': 4.7944e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0784, 'learning_rate': 4.794386564209953e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:57:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0971, 'learning_rate': 4.7933e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0971, 'learning_rate': 4.7933455756155536e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:58:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0625, 'learning_rate': 4.7923e-05, 'epoch': 0.39}\n",
      "{'loss': 0.0625, 'learning_rate': 4.7923020720865414e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:59:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1094, 'learning_rate': 4.7913e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1094, 'learning_rate': 4.791256054767245e-05, 'epoch': 0.39}\n",
      "02/14/2024 23:59:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.1449, 'learning_rate': 4.7902e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1449, 'learning_rate': 4.7902075248047515e-05, 'epoch': 0.39}\n",
      "02/15/2024 00:00:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.1123, 'learning_rate': 4.7892e-05, 'epoch': 0.40}\n",
      "{'loss': 0.1123, 'learning_rate': 4.7891564833489035e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:01:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0763, 'learning_rate': 4.7881e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0763, 'learning_rate': 4.788102931552294e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:01:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.1200, 'learning_rate': 4.7870e-05, 'epoch': 0.40}\n",
      "{'loss': 0.12, 'learning_rate': 4.787046870570274e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:02:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1021, 'learning_rate': 4.7860e-05, 'epoch': 0.40}\n",
      "{'loss': 0.1021, 'learning_rate': 4.785988301560944e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:02:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0856, 'learning_rate': 4.7849e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0856, 'learning_rate': 4.784927225685153e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:03:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0945, 'learning_rate': 4.7839e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0945, 'learning_rate': 4.783863644106502e-05, 'epoch': 0.4}\n",
      "[INFO|trainer.py:2936] 2024-02-15 00:03:30,131 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2000\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 00:03:30,160 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 00:03:30,160 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 00:04:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1081, 'learning_rate': 4.7828e-05, 'epoch': 0.40}\n",
      "{'loss': 0.1081, 'learning_rate': 4.782797557991339e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:04:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0642, 'learning_rate': 4.7817e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0642, 'learning_rate': 4.7817289685087577e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:05:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0875, 'learning_rate': 4.7807e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0875, 'learning_rate': 4.780657876830597e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:05:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0673, 'learning_rate': 4.7796e-05, 'epoch': 0.40}\n",
      "{'loss': 0.0673, 'learning_rate': 4.77958428413144e-05, 'epoch': 0.4}\n",
      "02/15/2024 00:06:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0811, 'learning_rate': 4.7785e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0811, 'learning_rate': 4.7785081915886134e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:07:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0990, 'learning_rate': 4.7774e-05, 'epoch': 0.41}\n",
      "{'loss': 0.099, 'learning_rate': 4.777429600382185e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:07:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0662, 'learning_rate': 4.7763e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0662, 'learning_rate': 4.776348511694961e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:08:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0911, 'learning_rate': 4.7753e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0911, 'learning_rate': 4.775264926712489e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:09:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0680, 'learning_rate': 4.7742e-05, 'epoch': 0.41}\n",
      "{'loss': 0.068, 'learning_rate': 4.774178846623053e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:09:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0959, 'learning_rate': 4.7731e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0959, 'learning_rate': 4.773090272617672e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:10:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0773, 'learning_rate': 4.7720e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0773, 'learning_rate': 4.7719992058901006e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:10:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0653, 'learning_rate': 4.7709e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0653, 'learning_rate': 4.770905647636828e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:11:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.1086, 'learning_rate': 4.7698e-05, 'epoch': 0.41}\n",
      "{'loss': 0.1086, 'learning_rate': 4.769809599057075e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:12:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0949, 'learning_rate': 4.7687e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0949, 'learning_rate': 4.7687110613527926e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:12:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0915, 'learning_rate': 4.7676e-05, 'epoch': 0.41}\n",
      "{'loss': 0.0915, 'learning_rate': 4.7676100357286624e-05, 'epoch': 0.41}\n",
      "02/15/2024 00:13:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0862, 'learning_rate': 4.7665e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0862, 'learning_rate': 4.7665065233920945e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:14:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.1396, 'learning_rate': 4.7654e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1396, 'learning_rate': 4.7654005255532244e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:14:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0836, 'learning_rate': 4.7643e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0836, 'learning_rate': 4.764292043424916e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:15:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1198, 'learning_rate': 4.7632e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1198, 'learning_rate': 4.7631810782227535e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:15:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.0838, 'learning_rate': 4.7621e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0838, 'learning_rate': 4.762067631165049e-05, 'epoch': 0.42}\n",
      "[INFO|trainer.py:2936] 2024-02-15 00:15:58,435 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2100\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 00:15:58,464 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 00:15:58,464 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2100/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 00:16:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0933, 'learning_rate': 4.7610e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0933, 'learning_rate': 4.760951703472832e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:17:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0864, 'learning_rate': 4.7598e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0864, 'learning_rate': 4.7598332963698545e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:17:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0580, 'learning_rate': 4.7587e-05, 'epoch': 0.42}\n",
      "{'loss': 0.058, 'learning_rate': 4.7587124110825875e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:18:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0807, 'learning_rate': 4.7576e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0807, 'learning_rate': 4.7575890488402185e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:19:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1249, 'learning_rate': 4.7565e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1249, 'learning_rate': 4.756463210874652e-05, 'epoch': 0.42}\n",
      "02/15/2024 00:19:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0652, 'learning_rate': 4.7553e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0652, 'learning_rate': 4.755334898420507e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:20:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0901, 'learning_rate': 4.7542e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0901, 'learning_rate': 4.7542041127151184e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:20:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.1047, 'learning_rate': 4.7531e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1047, 'learning_rate': 4.7530708549985287e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:21:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0952, 'learning_rate': 4.7519e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0952, 'learning_rate': 4.751935126513496e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:22:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0793, 'learning_rate': 4.7508e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0793, 'learning_rate': 4.7507969285054845e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:22:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1108, 'learning_rate': 4.7497e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1108, 'learning_rate': 4.749656262222668e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:23:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0761, 'learning_rate': 4.7485e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0761, 'learning_rate': 4.7485131289159276e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:24:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0848, 'learning_rate': 4.7474e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0848, 'learning_rate': 4.7473675298388495e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:24:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0636, 'learning_rate': 4.7462e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0636, 'learning_rate': 4.746219466247722e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:25:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0685, 'learning_rate': 4.7451e-05, 'epoch': 0.43}\n",
      "{'loss': 0.0685, 'learning_rate': 4.745068939401539e-05, 'epoch': 0.43}\n",
      "02/15/2024 00:25:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0687, 'learning_rate': 4.7439e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0687, 'learning_rate': 4.743915950561994e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:26:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0703, 'learning_rate': 4.7428e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0703, 'learning_rate': 4.742760500993481e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:27:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0714, 'learning_rate': 4.7416e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0714, 'learning_rate': 4.7416025919630904e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:27:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0700, 'learning_rate': 4.7404e-05, 'epoch': 0.44}\n",
      "{'loss': 0.07, 'learning_rate': 4.740442224740612e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:28:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.1288, 'learning_rate': 4.7393e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1288, 'learning_rate': 4.7392794005985326e-05, 'epoch': 0.44}\n",
      "[INFO|trainer.py:2936] 2024-02-15 00:28:27,193 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2200\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 00:28:27,221 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 00:28:27,221 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2200/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 00:29:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0984, 'learning_rate': 4.7381e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0984, 'learning_rate': 4.7381141208120296e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:29:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0823, 'learning_rate': 4.7369e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0823, 'learning_rate': 4.736946386658976e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:30:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0807, 'learning_rate': 4.7358e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0807, 'learning_rate': 4.735776199419935e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:30:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0626, 'learning_rate': 4.7346e-05, 'epoch': 0.44}\n",
      "{'loss': 0.0626, 'learning_rate': 4.73460356037816e-05, 'epoch': 0.44}\n",
      "02/15/2024 00:31:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0897, 'learning_rate': 4.7334e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0897, 'learning_rate': 4.733428470819594e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:32:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0608, 'learning_rate': 4.7323e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0608, 'learning_rate': 4.7322509320328675e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:32:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0696, 'learning_rate': 4.7311e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0696, 'learning_rate': 4.731070945309295e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:33:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0824, 'learning_rate': 4.7299e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0824, 'learning_rate': 4.7298885119428773e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:34:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0588, 'learning_rate': 4.7287e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0588, 'learning_rate': 4.7287036332302967e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:34:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1024, 'learning_rate': 4.7275e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1024, 'learning_rate': 4.72751631047092e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:35:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0665, 'learning_rate': 4.7263e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0665, 'learning_rate': 4.72632654496679e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:35:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0915, 'learning_rate': 4.7251e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0915, 'learning_rate': 4.725134338022631e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:36:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.0765, 'learning_rate': 4.7239e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0765, 'learning_rate': 4.723939690945846e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:37:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0695, 'learning_rate': 4.7227e-05, 'epoch': 0.45}\n",
      "{'loss': 0.0695, 'learning_rate': 4.7227426050465084e-05, 'epoch': 0.45}\n",
      "02/15/2024 00:37:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0912, 'learning_rate': 4.7215e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0912, 'learning_rate': 4.7215430816373726e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:38:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0705, 'learning_rate': 4.7203e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0705, 'learning_rate': 4.720341122033862e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:39:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0962, 'learning_rate': 4.7191e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0962, 'learning_rate': 4.719136727554072e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:39:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0583, 'learning_rate': 4.7179e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0583, 'learning_rate': 4.71792989951877e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:40:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0933, 'learning_rate': 4.7167e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0933, 'learning_rate': 4.716720639251392e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:40:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0607, 'learning_rate': 4.7155e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0607, 'learning_rate': 4.715508948078037e-05, 'epoch': 0.46}\n",
      "[INFO|trainer.py:2936] 2024-02-15 00:40:52,023 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2300\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 00:40:52,051 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 00:40:52,051 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2300/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 00:41:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0968, 'learning_rate': 4.7143e-05, 'epoch': 0.46}\n",
      "{'loss': 0.0968, 'learning_rate': 4.7142948273274754e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:42:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.1088, 'learning_rate': 4.7131e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1088, 'learning_rate': 4.713078278331138e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:42:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0690, 'learning_rate': 4.7119e-05, 'epoch': 0.46}\n",
      "{'loss': 0.069, 'learning_rate': 4.7118593024231214e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:43:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.1295, 'learning_rate': 4.7106e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1295, 'learning_rate': 4.710637900940181e-05, 'epoch': 0.46}\n",
      "02/15/2024 00:43:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0828, 'learning_rate': 4.7094e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0828, 'learning_rate': 4.709414075221734e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:44:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0860, 'learning_rate': 4.7082e-05, 'epoch': 0.47}\n",
      "{'loss': 0.086, 'learning_rate': 4.7081878266098545e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:45:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0959, 'learning_rate': 4.7070e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0959, 'learning_rate': 4.706959156449276e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:45:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.1305, 'learning_rate': 4.7057e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1305, 'learning_rate': 4.7057280660873835e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:46:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0866, 'learning_rate': 4.7045e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0866, 'learning_rate': 4.704494556874221e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:47:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0693, 'learning_rate': 4.7033e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0693, 'learning_rate': 4.70325863016248e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:47:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0832, 'learning_rate': 4.7020e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0832, 'learning_rate': 4.702020287307509e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:48:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0741, 'learning_rate': 4.7008e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0741, 'learning_rate': 4.7007795296673006e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:48:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0814, 'learning_rate': 4.6995e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0814, 'learning_rate': 4.6995363586024975e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:49:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0888, 'learning_rate': 4.6983e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0888, 'learning_rate': 4.6982907754763906e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:50:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0795, 'learning_rate': 4.6970e-05, 'epoch': 0.47}\n",
      "{'loss': 0.0795, 'learning_rate': 4.697042781654913e-05, 'epoch': 0.47}\n",
      "02/15/2024 00:50:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0929, 'learning_rate': 4.6958e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0929, 'learning_rate': 4.6957923785066445e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:51:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0760, 'learning_rate': 4.6945e-05, 'epoch': 0.48}\n",
      "{'loss': 0.076, 'learning_rate': 4.6945395674028046e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:52:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0512, 'learning_rate': 4.6933e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0512, 'learning_rate': 4.693284349717254e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:52:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.0837, 'learning_rate': 4.6920e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0837, 'learning_rate': 4.692026726826493e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:53:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0632, 'learning_rate': 4.6908e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0632, 'learning_rate': 4.690766700109659e-05, 'epoch': 0.48}\n",
      "[INFO|trainer.py:2936] 2024-02-15 00:53:18,135 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2400\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 00:53:18,164 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 00:53:18,164 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2400/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 00:53:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.1008, 'learning_rate': 4.6895e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1008, 'learning_rate': 4.689504270948527e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:54:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0847, 'learning_rate': 4.6882e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0847, 'learning_rate': 4.6882394407275044e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:55:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0867, 'learning_rate': 4.6870e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0867, 'learning_rate': 4.6869722108336323e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:55:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.1158, 'learning_rate': 4.6857e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1158, 'learning_rate': 4.685702582656584e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:56:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0873, 'learning_rate': 4.6844e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0873, 'learning_rate': 4.684430557588664e-05, 'epoch': 0.48}\n",
      "02/15/2024 00:57:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0865, 'learning_rate': 4.6832e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0865, 'learning_rate': 4.683156137024801e-05, 'epoch': 0.49}\n",
      "02/15/2024 00:57:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0635, 'learning_rate': 4.6819e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0635, 'learning_rate': 4.681879322362555e-05, 'epoch': 0.49}\n",
      "02/15/2024 00:58:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0819, 'learning_rate': 4.6806e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0819, 'learning_rate': 4.68060011500211e-05, 'epoch': 0.49}\n",
      "02/15/2024 00:58:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0768, 'learning_rate': 4.6793e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0768, 'learning_rate': 4.6793185163462726e-05, 'epoch': 0.49}\n",
      "02/15/2024 00:59:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0682, 'learning_rate': 4.6780e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0682, 'learning_rate': 4.678034527800474e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:00:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0813, 'learning_rate': 4.6767e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0813, 'learning_rate': 4.6767481507727646e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:00:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0589, 'learning_rate': 4.6755e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0589, 'learning_rate': 4.675459386673815e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:01:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0991, 'learning_rate': 4.6742e-05, 'epoch': 0.49}\n",
      "{'loss': 0.0991, 'learning_rate': 4.6741682369169116e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:02:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.1040, 'learning_rate': 4.6729e-05, 'epoch': 0.49}\n",
      "{'loss': 0.104, 'learning_rate': 4.67287470291796e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:02:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.1148, 'learning_rate': 4.6716e-05, 'epoch': 0.49}\n",
      "{'loss': 0.1148, 'learning_rate': 4.671578786095478e-05, 'epoch': 0.49}\n",
      "02/15/2024 01:03:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0528, 'learning_rate': 4.6703e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0528, 'learning_rate': 4.670280487870598e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:03:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0865, 'learning_rate': 4.6690e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0865, 'learning_rate': 4.6689798096670626e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:04:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0770, 'learning_rate': 4.6677e-05, 'epoch': 0.50}\n",
      "{'loss': 0.077, 'learning_rate': 4.667676752911225e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:05:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0941, 'learning_rate': 4.6664e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0941, 'learning_rate': 4.666371319032047e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:05:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.0796, 'learning_rate': 4.6651e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0796, 'learning_rate': 4.665063509461097e-05, 'epoch': 0.5}\n",
      "[INFO|trainer.py:2936] 2024-02-15 01:05:45,740 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2500\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 01:05:45,769 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 01:05:45,769 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 01:06:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0834, 'learning_rate': 4.6638e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0834, 'learning_rate': 4.663753325632548e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:07:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0768, 'learning_rate': 4.6624e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0768, 'learning_rate': 4.662440768983177e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:07:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0762, 'learning_rate': 4.6611e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0762, 'learning_rate': 4.661125840952364e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:08:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0564, 'learning_rate': 4.6598e-05, 'epoch': 0.50}\n",
      "{'loss': 0.0564, 'learning_rate': 4.659808542982088e-05, 'epoch': 0.5}\n",
      "02/15/2024 01:08:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.0785, 'learning_rate': 4.6585e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0785, 'learning_rate': 4.6584888765169296e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:09:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0897, 'learning_rate': 4.6572e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0897, 'learning_rate': 4.6571668430040625e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:10:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0499, 'learning_rate': 4.6558e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0499, 'learning_rate': 4.65584244389326e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:10:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.1038, 'learning_rate': 4.6545e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1038, 'learning_rate': 4.654515680636888e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:11:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0633, 'learning_rate': 4.6532e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0633, 'learning_rate': 4.6531865546899045e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:11:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0771, 'learning_rate': 4.6519e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0771, 'learning_rate': 4.65185506750986e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:12:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0770, 'learning_rate': 4.6505e-05, 'epoch': 0.51}\n",
      "{'loss': 0.077, 'learning_rate': 4.6505212205568916e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:13:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0688, 'learning_rate': 4.6492e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0688, 'learning_rate': 4.649185015293728e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:13:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1463, 'learning_rate': 4.6478e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1463, 'learning_rate': 4.647846453185681e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:14:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0998, 'learning_rate': 4.6465e-05, 'epoch': 0.51}\n",
      "{'loss': 0.0998, 'learning_rate': 4.646505535700649e-05, 'epoch': 0.51}\n",
      "02/15/2024 01:15:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.1016, 'learning_rate': 4.6452e-05, 'epoch': 0.52}\n",
      "{'loss': 0.1016, 'learning_rate': 4.645162264309112e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:15:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.0692, 'learning_rate': 4.6438e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0692, 'learning_rate': 4.643816640484131e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:16:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0912, 'learning_rate': 4.6425e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0912, 'learning_rate': 4.6424686657013484e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:16:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0741, 'learning_rate': 4.6411e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0741, 'learning_rate': 4.6411183414389837e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:17:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0927, 'learning_rate': 4.6398e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0927, 'learning_rate': 4.639765669177833e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:18:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0655, 'learning_rate': 4.6384e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0655, 'learning_rate': 4.638410650401267e-05, 'epoch': 0.52}\n",
      "[INFO|trainer.py:2936] 2024-02-15 01:18:12,215 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2600\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 01:18:12,243 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 01:18:12,243 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2600/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 01:18:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0980, 'learning_rate': 4.6371e-05, 'epoch': 0.52}\n",
      "{'loss': 0.098, 'learning_rate': 4.6370532865952296e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:19:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0939, 'learning_rate': 4.6357e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0939, 'learning_rate': 4.635693579248238e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:20:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0722, 'learning_rate': 4.6343e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0722, 'learning_rate': 4.6343315298513765e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:20:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.0760, 'learning_rate': 4.6330e-05, 'epoch': 0.52}\n",
      "{'loss': 0.076, 'learning_rate': 4.632967139898301e-05, 'epoch': 0.52}\n",
      "02/15/2024 01:21:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.0729, 'learning_rate': 4.6316e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0729, 'learning_rate': 4.6316004108852305e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:21:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0911, 'learning_rate': 4.6302e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0911, 'learning_rate': 4.6302313443109526e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:22:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0559, 'learning_rate': 4.6289e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0559, 'learning_rate': 4.6288599416768155e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:23:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.0844, 'learning_rate': 4.6275e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0844, 'learning_rate': 4.6274862044867304e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:23:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0630, 'learning_rate': 4.6261e-05, 'epoch': 0.53}\n",
      "{'loss': 0.063, 'learning_rate': 4.626110134247168e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:24:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0590, 'learning_rate': 4.6247e-05, 'epoch': 0.53}\n",
      "{'loss': 0.059, 'learning_rate': 4.6247317324671605e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:25:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0568, 'learning_rate': 4.6234e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0568, 'learning_rate': 4.6233510006582914e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:25:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.1163, 'learning_rate': 4.6220e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1163, 'learning_rate': 4.621967940334705e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:26:18 - INFO - llmtuner.extras.callbacks - {'loss': 0.1018, 'learning_rate': 4.6206e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1018, 'learning_rate': 4.6205825530130944e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:26:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.1016, 'learning_rate': 4.6192e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1016, 'learning_rate': 4.619194840212708e-05, 'epoch': 0.53}\n",
      "02/15/2024 01:27:33 - INFO - llmtuner.extras.callbacks - {'loss': 0.0714, 'learning_rate': 4.6178e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0714, 'learning_rate': 4.617804803455344e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:28:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.1050, 'learning_rate': 4.6164e-05, 'epoch': 0.54}\n",
      "{'loss': 0.105, 'learning_rate': 4.616412444265345e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:28:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0760, 'learning_rate': 4.6150e-05, 'epoch': 0.54}\n",
      "{'loss': 0.076, 'learning_rate': 4.6150177641696055e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:29:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0673, 'learning_rate': 4.6136e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0673, 'learning_rate': 4.613620764697564e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:30:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0578, 'learning_rate': 4.6122e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0578, 'learning_rate': 4.6122214473812005e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:30:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0871, 'learning_rate': 4.6108e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0871, 'learning_rate': 4.610819813755038e-05, 'epoch': 0.54}\n",
      "[INFO|trainer.py:2936] 2024-02-15 01:30:39,605 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2700\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 01:30:39,634 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2700/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 01:30:39,634 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2700/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 01:31:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.1179, 'learning_rate': 4.6094e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1179, 'learning_rate': 4.60941586535614e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:31:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0528, 'learning_rate': 4.6080e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0528, 'learning_rate': 4.6080096037241086e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:32:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0808, 'learning_rate': 4.6066e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0808, 'learning_rate': 4.606601030401081e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:33:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0771, 'learning_rate': 4.6052e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0771, 'learning_rate': 4.605190146931731e-05, 'epoch': 0.54}\n",
      "02/15/2024 01:33:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0612, 'learning_rate': 4.6038e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0612, 'learning_rate': 4.6037769548632656e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:34:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0709, 'learning_rate': 4.6024e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0709, 'learning_rate': 4.602361455745423e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:35:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0954, 'learning_rate': 4.6009e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0954, 'learning_rate': 4.6009436511304715e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:35:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0737, 'learning_rate': 4.5995e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0737, 'learning_rate': 4.599523542573207e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:36:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0601, 'learning_rate': 4.5981e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0601, 'learning_rate': 4.598101131630954e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:36:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0756, 'learning_rate': 4.5967e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0756, 'learning_rate': 4.5966764198635606e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:37:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0577, 'learning_rate': 4.5952e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0577, 'learning_rate': 4.5952494088333964e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:38:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0765, 'learning_rate': 4.5938e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0765, 'learning_rate': 4.593820100105355e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:38:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.1568, 'learning_rate': 4.5924e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1568, 'learning_rate': 4.592388495246848e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:39:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0932, 'learning_rate': 4.5910e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0932, 'learning_rate': 4.590954595827806e-05, 'epoch': 0.55}\n",
      "02/15/2024 01:40:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0662, 'learning_rate': 4.5895e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0662, 'learning_rate': 4.5895184034206765e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:40:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0673, 'learning_rate': 4.5881e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0673, 'learning_rate': 4.588079919600419e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:41:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1182, 'learning_rate': 4.5866e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1182, 'learning_rate': 4.586639145944508e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:41:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0939, 'learning_rate': 4.5852e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0939, 'learning_rate': 4.585196084032928e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:42:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0819, 'learning_rate': 4.5838e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0819, 'learning_rate': 4.5837507354481745e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:43:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0921, 'learning_rate': 4.5823e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0921, 'learning_rate': 4.5823031017752485e-05, 'epoch': 0.56}\n",
      "[INFO|trainer.py:2936] 2024-02-15 01:43:08,036 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2800\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 01:43:08,064 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2800/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 01:43:08,064 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2800/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 01:43:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.1346, 'learning_rate': 4.5809e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1346, 'learning_rate': 4.580853184601659e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:44:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0833, 'learning_rate': 4.5794e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0833, 'learning_rate': 4.579400985517416e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:45:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0844, 'learning_rate': 4.5779e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0844, 'learning_rate': 4.577946506115035e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:45:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0864, 'learning_rate': 4.5765e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0864, 'learning_rate': 4.5764897479895317e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:46:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0820, 'learning_rate': 4.5750e-05, 'epoch': 0.56}\n",
      "{'loss': 0.082, 'learning_rate': 4.575030712738419e-05, 'epoch': 0.56}\n",
      "02/15/2024 01:46:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0629, 'learning_rate': 4.5736e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0629, 'learning_rate': 4.573569401961708e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:47:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0841, 'learning_rate': 4.5721e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0841, 'learning_rate': 4.572105817261905e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:48:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.0695, 'learning_rate': 4.5706e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0695, 'learning_rate': 4.5706399602440106e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:48:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0912, 'learning_rate': 4.5692e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0912, 'learning_rate': 4.569171832515516e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:49:20 - INFO - llmtuner.extras.callbacks - {'loss': 0.0877, 'learning_rate': 4.5677e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0877, 'learning_rate': 4.567701435686404e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:49:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.0511, 'learning_rate': 4.5662e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0511, 'learning_rate': 4.566228771369146e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:50:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0829, 'learning_rate': 4.5648e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0829, 'learning_rate': 4.564753841178697e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:51:13 - INFO - llmtuner.extras.callbacks - {'loss': 0.0753, 'learning_rate': 4.5633e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0753, 'learning_rate': 4.563276646732499e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:51:50 - INFO - llmtuner.extras.callbacks - {'loss': 0.0777, 'learning_rate': 4.5618e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0777, 'learning_rate': 4.561797189650478e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:52:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0995, 'learning_rate': 4.5603e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0995, 'learning_rate': 4.5603154715550386e-05, 'epoch': 0.57}\n",
      "02/15/2024 01:53:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0939, 'learning_rate': 4.5588e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0939, 'learning_rate': 4.558831494071069e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:53:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.0738, 'learning_rate': 4.5573e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0738, 'learning_rate': 4.5573452588259295e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:54:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0742, 'learning_rate': 4.5559e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0742, 'learning_rate': 4.555856767449461e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:54:56 - INFO - llmtuner.extras.callbacks - {'loss': 0.0704, 'learning_rate': 4.5544e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0704, 'learning_rate': 4.554366021573976e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:55:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0833, 'learning_rate': 4.5529e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0833, 'learning_rate': 4.5528730228342605e-05, 'epoch': 0.58}\n",
      "[INFO|trainer.py:2936] 2024-02-15 01:55:34,440 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-2900\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 01:55:34,469 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2900/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 01:55:34,469 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-2900/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 01:56:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.1044, 'learning_rate': 4.5514e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1044, 'learning_rate': 4.551377772867571e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:56:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.0496, 'learning_rate': 4.5499e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0496, 'learning_rate': 4.549880273313631e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:57:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.0862, 'learning_rate': 4.5484e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0862, 'learning_rate': 4.548380525814634e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:58:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0796, 'learning_rate': 4.5469e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0796, 'learning_rate': 4.5468785320152365e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:58:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0555, 'learning_rate': 4.5454e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0555, 'learning_rate': 4.545374293562559e-05, 'epoch': 0.58}\n",
      "02/15/2024 01:59:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0722, 'learning_rate': 4.5439e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0722, 'learning_rate': 4.543867812106183e-05, 'epoch': 0.59}\n",
      "02/15/2024 01:59:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0947, 'learning_rate': 4.5424e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0947, 'learning_rate': 4.5423590892981506e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:00:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0646, 'learning_rate': 4.5408e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0646, 'learning_rate': 4.5408481267929605e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:01:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0944, 'learning_rate': 4.5393e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0944, 'learning_rate': 4.539334926247569e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:01:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0706, 'learning_rate': 4.5378e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0706, 'learning_rate': 4.537819489321386e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:02:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0761, 'learning_rate': 4.5363e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0761, 'learning_rate': 4.536301817676274e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:03:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0792, 'learning_rate': 4.5348e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0792, 'learning_rate': 4.534781912976546e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:03:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.1115, 'learning_rate': 4.5333e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1115, 'learning_rate': 4.533259776888963e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:04:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0533, 'learning_rate': 4.5317e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0533, 'learning_rate': 4.531735411082735e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:04:54 - INFO - llmtuner.extras.callbacks - {'loss': 0.0644, 'learning_rate': 4.5302e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0644, 'learning_rate': 4.5302088172295156e-05, 'epoch': 0.59}\n",
      "02/15/2024 02:05:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0991, 'learning_rate': 4.5287e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0991, 'learning_rate': 4.528679997003403e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:06:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0822, 'learning_rate': 4.5271e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0822, 'learning_rate': 4.527148952080934e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:06:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0901, 'learning_rate': 4.5256e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0901, 'learning_rate': 4.5256156841410886e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:07:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.0614, 'learning_rate': 4.5241e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0614, 'learning_rate': 4.524080194865283e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:08:02 - INFO - llmtuner.extras.callbacks - {'loss': 0.0683, 'learning_rate': 4.5225e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0683, 'learning_rate': 4.522542485937369e-05, 'epoch': 0.6}\n",
      "[INFO|trainer.py:2936] 2024-02-15 02:08:02,141 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-3000\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 02:08:02,170 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 02:08:02,170 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 02:08:40 - INFO - llmtuner.extras.callbacks - {'loss': 0.0810, 'learning_rate': 4.5210e-05, 'epoch': 0.60}\n",
      "{'loss': 0.081, 'learning_rate': 4.5210025590436334e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:09:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0654, 'learning_rate': 4.5195e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0654, 'learning_rate': 4.519460415872794e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:09:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0769, 'learning_rate': 4.5179e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0769, 'learning_rate': 4.5179160581160005e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:10:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.0544, 'learning_rate': 4.5164e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0544, 'learning_rate': 4.516369487466832e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:11:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0561, 'learning_rate': 4.5148e-05, 'epoch': 0.60}\n",
      "{'loss': 0.0561, 'learning_rate': 4.5148207056212896e-05, 'epoch': 0.6}\n",
      "02/15/2024 02:11:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0729, 'learning_rate': 4.5133e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0729, 'learning_rate': 4.513269714277805e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:12:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0933, 'learning_rate': 4.5117e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0933, 'learning_rate': 4.5117165151372296e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:13:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0660, 'learning_rate': 4.5102e-05, 'epoch': 0.61}\n",
      "{'loss': 0.066, 'learning_rate': 4.510161109902837e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:13:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0685, 'learning_rate': 4.5086e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0685, 'learning_rate': 4.5086035002803195e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:14:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1339, 'learning_rate': 4.5070e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1339, 'learning_rate': 4.5070436879777865e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:14:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.1005, 'learning_rate': 4.5055e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1005, 'learning_rate': 4.5054816747057647e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:15:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0741, 'learning_rate': 4.5039e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0741, 'learning_rate': 4.503917462177192e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:16:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0732, 'learning_rate': 4.5024e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0732, 'learning_rate': 4.50235105210742e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:16:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.0946, 'learning_rate': 4.5008e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0946, 'learning_rate': 4.5007824462142076e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:17:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.1168, 'learning_rate': 4.4992e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1168, 'learning_rate': 4.499211646217727e-05, 'epoch': 0.61}\n",
      "02/15/2024 02:18:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0517, 'learning_rate': 4.4976e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0517, 'learning_rate': 4.4976386538405495e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:18:38 - INFO - llmtuner.extras.callbacks - {'loss': 0.0506, 'learning_rate': 4.4961e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0506, 'learning_rate': 4.4960634708076566e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:19:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.0891, 'learning_rate': 4.4945e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0891, 'learning_rate': 4.4944860988464276e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:19:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0860, 'learning_rate': 4.4929e-05, 'epoch': 0.62}\n",
      "{'loss': 0.086, 'learning_rate': 4.492906539686646e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:20:30 - INFO - llmtuner.extras.callbacks - {'loss': 0.0710, 'learning_rate': 4.4913e-05, 'epoch': 0.62}\n",
      "{'loss': 0.071, 'learning_rate': 4.491324795060491e-05, 'epoch': 0.62}\n",
      "[INFO|trainer.py:2936] 2024-02-15 02:20:30,751 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-3100\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 02:20:30,781 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 02:20:30,781 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3100/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 02:21:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0844, 'learning_rate': 4.4897e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0844, 'learning_rate': 4.48974086670254e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:21:45 - INFO - llmtuner.extras.callbacks - {'loss': 0.0783, 'learning_rate': 4.4882e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0783, 'learning_rate': 4.488154756349764e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:22:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.1016, 'learning_rate': 4.4866e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1016, 'learning_rate': 4.4865664657415286e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:23:00 - INFO - llmtuner.extras.callbacks - {'loss': 0.0408, 'learning_rate': 4.4850e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0408, 'learning_rate': 4.484975996619589e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:23:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0697, 'learning_rate': 4.4834e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0697, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62}\n",
      "02/15/2024 02:24:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0690, 'learning_rate': 4.4818e-05, 'epoch': 0.63}\n",
      "{'loss': 0.069, 'learning_rate': 4.481788529813559e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:24:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1002, 'learning_rate': 4.4802e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1002, 'learning_rate': 4.480191535624918e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:25:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0788, 'learning_rate': 4.4786e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0788, 'learning_rate': 4.478592369913465e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:26:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0538, 'learning_rate': 4.4770e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0538, 'learning_rate': 4.47699103443288e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:26:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.1084, 'learning_rate': 4.4754e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1084, 'learning_rate': 4.4753875309392266e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:27:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0703, 'learning_rate': 4.4738e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0703, 'learning_rate': 4.47378186119094e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:27:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0477, 'learning_rate': 4.4722e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0477, 'learning_rate': 4.4721740269488355e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:28:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.1248, 'learning_rate': 4.4706e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1248, 'learning_rate': 4.4705640299761007e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:29:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0881, 'learning_rate': 4.4690e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0881, 'learning_rate': 4.4689518720382937e-05, 'epoch': 0.63}\n",
      "02/15/2024 02:29:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0513, 'learning_rate': 4.4673e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0513, 'learning_rate': 4.467337554903344e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:30:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0793, 'learning_rate': 4.4657e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0793, 'learning_rate': 4.465721080341547e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:31:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.1044, 'learning_rate': 4.4641e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1044, 'learning_rate': 4.464102450125568e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:31:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0743, 'learning_rate': 4.4625e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0743, 'learning_rate': 4.4624816660304314e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:32:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0847, 'learning_rate': 4.4609e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0847, 'learning_rate': 4.460858729833525e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:32:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0551, 'learning_rate': 4.4592e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0551, 'learning_rate': 4.4592336433146e-05, 'epoch': 0.64}\n",
      "[INFO|trainer.py:2936] 2024-02-15 02:32:59,044 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-3200\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 02:32:59,075 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 02:32:59,075 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3200/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 02:33:36 - INFO - llmtuner.extras.callbacks - {'loss': 0.0775, 'learning_rate': 4.4576e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0775, 'learning_rate': 4.457606408255761e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:34:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0661, 'learning_rate': 4.4560e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0661, 'learning_rate': 4.45597702644147e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:34:52 - INFO - llmtuner.extras.callbacks - {'loss': 0.1021, 'learning_rate': 4.4543e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1021, 'learning_rate': 4.454345499658546e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:35:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.0788, 'learning_rate': 4.4527e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0788, 'learning_rate': 4.452711829696158e-05, 'epoch': 0.64}\n",
      "02/15/2024 02:36:07 - INFO - llmtuner.extras.callbacks - {'loss': 0.0737, 'learning_rate': 4.4511e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0737, 'learning_rate': 4.451076018345825e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:36:44 - INFO - llmtuner.extras.callbacks - {'loss': 0.0763, 'learning_rate': 4.4494e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0763, 'learning_rate': 4.449438067401413e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:37:22 - INFO - llmtuner.extras.callbacks - {'loss': 0.0962, 'learning_rate': 4.4478e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0962, 'learning_rate': 4.4477979786591386e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:37:59 - INFO - llmtuner.extras.callbacks - {'loss': 0.0739, 'learning_rate': 4.4462e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0739, 'learning_rate': 4.4461557539175594e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:38:37 - INFO - llmtuner.extras.callbacks - {'loss': 0.0856, 'learning_rate': 4.4445e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0856, 'learning_rate': 4.4445113949775755e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:39:14 - INFO - llmtuner.extras.callbacks - {'loss': 0.0921, 'learning_rate': 4.4429e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0921, 'learning_rate': 4.442864903642428e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:39:51 - INFO - llmtuner.extras.callbacks - {'loss': 0.0748, 'learning_rate': 4.4412e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0748, 'learning_rate': 4.4412162817176965e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:40:28 - INFO - llmtuner.extras.callbacks - {'loss': 0.0669, 'learning_rate': 4.4396e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0669, 'learning_rate': 4.439565531011299e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:41:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.0679, 'learning_rate': 4.4379e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0679, 'learning_rate': 4.437912653333484e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:41:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.0447, 'learning_rate': 4.4363e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0447, 'learning_rate': 4.436257650496834e-05, 'epoch': 0.65}\n",
      "02/15/2024 02:42:21 - INFO - llmtuner.extras.callbacks - {'loss': 0.0927, 'learning_rate': 4.4346e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0927, 'learning_rate': 4.434600524316266e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:42:58 - INFO - llmtuner.extras.callbacks - {'loss': 0.0990, 'learning_rate': 4.4329e-05, 'epoch': 0.66}\n",
      "{'loss': 0.099, 'learning_rate': 4.432941276609018e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:43:35 - INFO - llmtuner.extras.callbacks - {'loss': 0.0893, 'learning_rate': 4.4313e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0893, 'learning_rate': 4.431279909194661e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:44:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0784, 'learning_rate': 4.4296e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0784, 'learning_rate': 4.4296164238950874e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:44:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.0979, 'learning_rate': 4.4280e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0979, 'learning_rate': 4.427950822534513e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:45:27 - INFO - llmtuner.extras.callbacks - {'loss': 0.0613, 'learning_rate': 4.4263e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0613, 'learning_rate': 4.426283106939474e-05, 'epoch': 0.66}\n",
      "[INFO|trainer.py:2936] 2024-02-15 02:45:27,500 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-3300\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 02:45:27,530 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 02:45:27,530 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3300/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 02:46:05 - INFO - llmtuner.extras.callbacks - {'loss': 0.1028, 'learning_rate': 4.4246e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1028, 'learning_rate': 4.424613278938823e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:46:42 - INFO - llmtuner.extras.callbacks - {'loss': 0.0751, 'learning_rate': 4.4229e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0751, 'learning_rate': 4.4229413403637345e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:47:19 - INFO - llmtuner.extras.callbacks - {'loss': 0.0891, 'learning_rate': 4.4213e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0891, 'learning_rate': 4.4212672930476915e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:47:57 - INFO - llmtuner.extras.callbacks - {'loss': 0.0788, 'learning_rate': 4.4196e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0788, 'learning_rate': 4.4195911388264946e-05, 'epoch': 0.66}\n",
      "02/15/2024 02:48:34 - INFO - llmtuner.extras.callbacks - {'loss': 0.0762, 'learning_rate': 4.4179e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0762, 'learning_rate': 4.41791287953825e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:49:12 - INFO - llmtuner.extras.callbacks - {'loss': 0.0964, 'learning_rate': 4.4162e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0964, 'learning_rate': 4.4162325170233745e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:49:49 - INFO - llmtuner.extras.callbacks - {'loss': 0.1018, 'learning_rate': 4.4146e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1018, 'learning_rate': 4.4145500531245945e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:50:26 - INFO - llmtuner.extras.callbacks - {'loss': 0.0752, 'learning_rate': 4.4129e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0752, 'learning_rate': 4.412865489686936e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:51:04 - INFO - llmtuner.extras.callbacks - {'loss': 0.0744, 'learning_rate': 4.4112e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0744, 'learning_rate': 4.4111788285577294e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:51:41 - INFO - llmtuner.extras.callbacks - {'loss': 0.1144, 'learning_rate': 4.4095e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1144, 'learning_rate': 4.4094900715866064e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:52:17 - INFO - llmtuner.extras.callbacks - {'loss': 0.0543, 'learning_rate': 4.4078e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0543, 'learning_rate': 4.4077992206254934e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:52:55 - INFO - llmtuner.extras.callbacks - {'loss': 0.0556, 'learning_rate': 4.4061e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0556, 'learning_rate': 4.40610627752862e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:53:32 - INFO - llmtuner.extras.callbacks - {'loss': 0.0785, 'learning_rate': 4.4044e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0785, 'learning_rate': 4.4044112441525024e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:54:09 - INFO - llmtuner.extras.callbacks - {'loss': 0.0701, 'learning_rate': 4.4027e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0701, 'learning_rate': 4.402714122355955e-05, 'epoch': 0.67}\n",
      "02/15/2024 02:54:47 - INFO - llmtuner.extras.callbacks - {'loss': 0.0767, 'learning_rate': 4.4010e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0767, 'learning_rate': 4.401014914000078e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:55:24 - INFO - llmtuner.extras.callbacks - {'loss': 0.1046, 'learning_rate': 4.3993e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1046, 'learning_rate': 4.399313620948262e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:56:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.0789, 'learning_rate': 4.3976e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0789, 'learning_rate': 4.3976102450661846e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:56:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.0855, 'learning_rate': 4.3959e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0855, 'learning_rate': 4.395904788221805e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:57:16 - INFO - llmtuner.extras.callbacks - {'loss': 0.0745, 'learning_rate': 4.3942e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0745, 'learning_rate': 4.394197252285366e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:57:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0433, 'learning_rate': 4.3925e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0433, 'learning_rate': 4.3924876391293915e-05, 'epoch': 0.68}\n",
      "[INFO|trainer.py:2936] 2024-02-15 02:57:53,727 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007/tmp-checkpoint-3400\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 02:57:53,755 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 02:57:53,755 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/tmp-checkpoint-3400/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "02/15/2024 02:58:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0696, 'learning_rate': 4.3908e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0696, 'learning_rate': 4.39077595062868e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:59:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.0844, 'learning_rate': 4.3891e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0844, 'learning_rate': 4.389062188660309e-05, 'epoch': 0.68}\n",
      "02/15/2024 02:59:46 - INFO - llmtuner.extras.callbacks - {'loss': 0.0733, 'learning_rate': 4.3873e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0733, 'learning_rate': 4.3873463551036286e-05, 'epoch': 0.68}\n",
      "02/15/2024 03:00:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0409, 'learning_rate': 4.3856e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0409, 'learning_rate': 4.3856284518402594e-05, 'epoch': 0.68}\n",
      "02/15/2024 03:01:01 - INFO - llmtuner.extras.callbacks - {'loss': 0.1157, 'learning_rate': 4.3839e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1157, 'learning_rate': 4.383908480754095e-05, 'epoch': 0.69}\n",
      "02/15/2024 03:01:39 - INFO - llmtuner.extras.callbacks - {'loss': 0.1041, 'learning_rate': 4.3822e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1041, 'learning_rate': 4.382186443731293e-05, 'epoch': 0.69}\n",
      "02/15/2024 03:02:15 - INFO - llmtuner.extras.callbacks - {'loss': 0.1024, 'learning_rate': 4.3805e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1024, 'learning_rate': 4.3804623426602784e-05, 'epoch': 0.69}\n",
      "02/15/2024 03:02:53 - INFO - llmtuner.extras.callbacks - {'loss': 0.0906, 'learning_rate': 4.3787e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0906, 'learning_rate': 4.3787361794317405e-05, 'epoch': 0.69}\n",
      "02/15/2024 03:03:31 - INFO - llmtuner.extras.callbacks - {'loss': 0.0500, 'learning_rate': 4.3770e-05, 'epoch': 0.69}\n",
      "{'loss': 0.05, 'learning_rate': 4.3770079559386276e-05, 'epoch': 0.69}\n",
      "02/15/2024 03:04:08 - INFO - llmtuner.extras.callbacks - {'loss': 0.1090, 'learning_rate': 4.3753e-05, 'epoch': 0.69}\n",
      "{'loss': 0.109, 'learning_rate': 4.375277674076149e-05, 'epoch': 0.69}\n",
      "[INFO|trainer.py:1962] 2024-02-15 03:04:23,704 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/15/2024 03:04:23 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 0.69}\n",
      "{'train_runtime': 25801.7657, 'train_samples_per_second': 4.651, 'train_steps_per_second': 0.581, 'train_loss': 0.09768476354461142, 'epoch': 0.69}\n",
      "[INFO|trainer.py:2936] 2024-02-15 03:04:23,706 >> Saving model checkpoint to saves/Qwen-7B/lora/king_007\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-02-15 03:04:23,736 >> tokenizer config file saved in saves/Qwen-7B/lora/king_007/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-02-15 03:04:23,736 >> Special tokens file saved in saves/Qwen-7B/lora/king_007/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.69\n",
      "  train_loss               =     0.0977\n",
      "  train_runtime            = 7:10:01.76\n",
      "  train_samples_per_second =      4.651\n",
      "  train_steps_per_second   =      0.581\n",
      "[INFO|modelcard.py:452] 2024-02-15 03:04:23,834 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "2024-02-15 03:04:26,174 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-02-15 03:04:26,174 - modelscope - INFO - Use user-specified model revision: master\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:04:26,882 >> loading file qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:04:26,882 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:04:26,882 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:04:26,882 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:04:26,882 >> loading file tokenizer.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-15 03:04:27,141 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-15 03:04:27,141 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 03:04:27,142 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/qwen/Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-15 03:04:27,147 >> loading weights file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 03:04:27,148 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 03:04:27,148 >> Generate config GenerationConfig {}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:41<00:00,  5.18s/it]\n",
      "[INFO|modeling_utils.py:4350] 2024-02-15 03:05:08,900 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4358] 2024-02-15 03:05:08,900 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at /mnt/workspace/.cache/modelscope/qwen/Qwen-7B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:779] 2024-02-15 03:05:08,903 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/generation_config.json\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 03:05:08,903 >> Generate config GenerationConfig {\n",
      "  \"chat_format\": \"raw\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"stop_words_ids\": [\n",
      "    [\n",
      "      151643\n",
      "    ]\n",
      "  ],\n",
      "  \"top_k\": 0,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "02/15/2024 03:05:08 - INFO - llmtuner.model.adapter - Adapter is not found at evaluation, load the base model.\n",
      "02/15/2024 03:05:08 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 7721324544 || trainable%: 0.0000\n",
      "02/15/2024 03:05:08 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
      "02/15/2024 03:05:11 - INFO - llmtuner.data.template - Add eos token: <|endoftext|>\n",
      "02/15/2024 03:05:11 - WARNING - llmtuner.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
      "02/15/2024 03:05:11 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
      "Exception in thread Thread-17 (generate):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1525, in generate\n",
      "    return self.sample(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2622, in sample\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py\", line 1043, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py\", line 891, in forward\n",
      "    outputs = block(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py\", line 629, in forward\n",
      "    mlp_output = self.mlp(layernorm_output)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/huggingface/modules/transformers_modules/Qwen-7B/modeling_qwen.py\", line 573, in forward\n",
      "    intermediate_parallel = a1 * F.silu(a2)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 2072, in silu\n",
      "    return torch._C._nn.silu(input)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacty of 22.20 GiB of which 115.12 MiB is free. Process 205794 has 22.08 GiB memory in use. Of the allocated memory 20.18 GiB is allocated by PyTorch, and 486.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/webui/chatter.py\", line 120, in predict\n",
      "    for new_text in self.stream_chat(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 35, in generator_context\n",
      "    response = gen.send(None)\n",
      "  File \"/mnt/workspace/LLaMA-Factory/src/llmtuner/chat/chat_model.py\", line 139, in stream_chat\n",
      "    yield from streamer\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/streamers.py\", line 223, in __next__\n",
      "    value = self.text_queue.get(timeout=self.timeout)\n",
      "  File \"/opt/conda/lib/python3.10/queue.py\", line 179, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "2024-02-15 03:09:59,274 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-02-15 03:09:59,274 - modelscope - INFO - Use user-specified model revision: master\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:09:59,978 >> loading file qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:09:59,978 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:09:59,978 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:09:59,978 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2025] 2024-02-15 03:09:59,979 >> loading file tokenizer.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-15 03:10:00,232 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:727] 2024-02-15 03:10:00,233 >> loading configuration file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-15 03:10:00,234 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/qwen/Qwen-7B\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3473] 2024-02-15 03:10:00,240 >> loading weights file /mnt/workspace/.cache/modelscope/qwen/Qwen-7B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1426] 2024-02-15 03:10:00,241 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:826] 2024-02-15 03:10:00,241 >> Generate config GenerationConfig {}\n",
      "\n",
      "Loading checkpoint shards:  12%|██▎               | 1/8 [00:04<00:32,  4.64s/it]"
     ]
    }
   ],
   "source": [
    "# !CUDA_VISIBLE_DEVICES=0 python src/train_web.py\n",
    "!CUDA_VISIBLE_DEVICES=0 USE_MODELSCOPE_HUB=1 python src/train_web.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
