from modelscope import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
from loguru import logger
import sqlite3
import torch
from get_prompt import process_prompt

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("j869903116/mrking_qwn1.5_7B_chat_text_to_sql", revision='master')
model = AutoModelForCausalLM.from_pretrained("j869903116/mrking_qwn1.5_7B_chat_text_to_sql", revision='master',
                                             device_map="auto", torch_dtype="auto").eval()


def execute_sql(sql_query):
    conn = sqlite3.connect('../../../test_gaokao_data/schools/school_detail.sqlite')
    cursor = conn.cursor()
    try:
        cursor.execute(sql_query)
        result = cursor.fetchall()
        return result
    except Exception as e:
        return str(e)


def predict(input, chatbot, history):
    try:
        chatbot.append((input, "Processing..."))
        prompt = process_prompt(input)
        model_inputs = tokenizer([prompt], return_tensors="pt").to(device)
        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        sql_query = response
        result = execute_sql(sql_query)
        if not result or result == "[]":
            model_inputs = tokenizer([input], return_tensors="pt").to(device)
            generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        else:
            response = "ä½ çš„æé—®æ–¹å¼ä¼¼ä¹å­˜åœ¨é—®é¢˜å“¦ï¼Œè¯·æ¢ç§æ–¹å¼æé—®è¯•è¯•ã€‚"
        chatbot[-1] = (str(input), str(response))  # Ensure chatbot is a list of tuples of strings
    except Exception as e:
        logger.error(f"Error in generating response: {str(e)}")
        chatbot[-1] = (str(input), "An error occurred while processing your request.")
    return chatbot, history


def reset_user_input():
    return gr.update(value="")


def reset_state(chatbot, task_history):
    chatbot.clear()
    task_history.clear()
    return chatbot


def regenerate(_chatbot, _task_history):
    if not _task_history:
        yield _chatbot
        return
    item = _task_history.pop(-1)
    _chatbot.pop(-1)
    yield from predict(item[0], _chatbot, _task_history)


def create_app_interface():
    # https: // qianwen - res.oss - cn - beijing.aliyuncs.com / logo_qwen.jpg
    with gr.Blocks() as demo:
        gr.Markdown("""\
        <p align="center"><img src="https://i.postimg.cc/gJ1sP6nj/image.png" style="height: 120px"/><p>""")
        # gr.Markdown("""<center><font size=8>åŸºäºText-to-SQLçš„é«˜è€ƒå¿—æ„¿å¡«æŠ¥è¾…åŠ©ç³»ç»Ÿ</center>""")
        gr.Markdown("""<center><b><font size=6 face='Sans-serif'>ğŸ“ğŸ“åŸºäºText-to-SQLçš„é«˜è€ƒå¿—æ„¿å¡«æŠ¥è¾…åŠ©ç³»ç»ŸğŸ“ğŸ“</font></b></center>""")
        gr.Markdown(
            """\
        <center><font size=3>(This WebUI is based on Qwen-Chat, developed by Alibaba Cloud. \
        (æœ¬WebUIåŸºäºQwen-Chatæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>""")
        gr.Markdown("""\
        <center><font size=4>ğŸ’ğŸ’ğŸ’  Github
        &nbsp<a href="https://github.com/tking007/hugging_face_test.git">Github</a></center>""")

        # chatbot = gr.Chatbot(label='ğŸ’‹ğŸ’‹ğŸ’‹', elem_classes="control-height")
        chatbot = gr.Chatbot(label='answer', height=300)
        query = gr.Textbox(lines=2, label='Input')
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            submit_btn = gr.Button("ğŸš€ Submit (å‘é€)")
            regen_btn = gr.Button("ğŸ¤” Regenerate (é‡è¯•)")

        submit_btn.click(predict, [query, chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(reset_user_input, [], [query])
        empty_btn.click(reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)

        gr.Markdown("""\
        <font size=2>
        (â˜ï¸â˜ï¸â˜ï¸æ³¨ï¼šæœ¬åº”ç”¨ç¨‹åºä½¿ç”¨çš„å¤§æ¨¡å‹å¯èƒ½å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¯¹æŸäº›é—®é¢˜çš„ç†è§£å’Œå›ç­”å¯èƒ½ä¸å‡†ç¡®ã€‚æˆ‘ä»¬å»ºè®®ç”¨æˆ·åœ¨å¾—åˆ°é”™è¯¯ç­”æ¡ˆæ—¶ï¼Œå°è¯•ä»¥ä¸åŒçš„æ–¹å¼æé—®ï¼Œæˆ–è€…ç”¨æ›´å…·ä½“çš„æ–¹å¼æè¿°é—®é¢˜ï¼Œä»¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å’Œå›ç­”ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äº:ğŸš«â›”ä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)
        """)

    return demo


if __name__ == "__main__":
    demo = create_app_interface()
    demo.queue().launch(server_name="0.0.0.0", server_port=None, share=True, inbrowser=True, debug=True)
