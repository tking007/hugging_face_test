{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      },
      "source": [
        "# LLaMA Factory Colab Tutorial\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhdAm2BMrHRV"
      },
      "source": [
        "### **这是我llama board项目的官方colab教程，复制过来的副本。--mrking**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr7rB3szzhtx"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH"
      },
      "outputs": [],
      "source": [
        "%rm -rf LLaMA-Factory\n",
        "!git clone https://github.com/tking007/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RXn_YQnn9f"
      },
      "source": [
        "### Check GPU environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkN-ktlsnrdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b58e1d8-59d5-404a-bd5a-7ee8b90087ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okkbTMoZCQNf"
      },
      "source": [
        "### Log in with Hugging Face account to upload model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OIm0O7oA5sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52dc41ea-ae83-481c-cc29-19d2518509e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "export HF_ENDPOINT=https://hf-mirror.com\n",
        "\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/saves /content/LLaMA-Factory"
      ],
      "metadata": {
        "id": "rjdKViPiv_-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RCmBi2wqC7kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sftp -P 15022 17674520445@192.168.20.150    智星云网盘内网地址"
      ],
      "metadata": {
        "id": "crqIOwkdsKCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export http_proxy=socks5://172.17.0.1:10808 && export https_proxy=socks5://172.17.0.1:10808   炼丹侠加速"
      ],
      "metadata": {
        "id": "OlSfx0iSmgu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "下载文件\n",
        "wget https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
        "重命名文件\n",
        "mv frpc_linux_amd64 frpc_linux_amd64_v0.2\n",
        "移动文件\n",
        "mv frpc_linux_amd64_v0.2 /home/vipuser/anaconda3/lib/python3.9/site-packages/gradio"
      ],
      "metadata": {
        "id": "qqyFgbBWDA5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python Qwen_web_demo.py --share --inbrowser --server_port=20238"
      ],
      "metadata": {
        "id": "Q1Sabsa_RCRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsof -i :6666"
      ],
      "metadata": {
        "id": "90TCoW2KpsAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "watch -n 5 nvidia-smi"
      ],
      "metadata": {
        "id": "AECeyVlopsCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sudo apt-get update\n",
        "sudo apt-get install psmisc\n",
        "fuser -v /dev/nvidia*"
      ],
      "metadata": {
        "id": "PxB6VHUiX4Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall gradio\n",
        "rm -rf /home/vipuser/anaconda3/lib/python3.9/site-packages/gradio\n",
        "pip install gradio==4.25"
      ],
      "metadata": {
        "id": "7PEnQND1wmeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      },
      "source": [
        "## Fine-tune model via LLaMA Board"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
      ],
      "metadata": {
        "id": "aPSEJZwQLcbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-CB_hIfvLhf"
      },
      "outputs": [],
      "source": [
        "# !CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLsdS6V5yUMy"
      },
      "outputs": [],
      "source": [
        "from llmtuner import create_ui\n",
        "\n",
        "create_ui().queue().launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "_16X7REJwzn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers_stream_generator"
      ],
      "metadata": {
        "id": "PgQG8EnrxMyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A100 --80G\n",
        "\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen1.5-7B-Chat \\\n",
        "    --finetuning_type lora \\\n",
        "    --template default \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset train_data_for_qwen_hf \\\n",
        "    --cutoff_len 3000 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 40000 \\\n",
        "    --per_device_train_batch_size 30 \\\n",
        "    --gradient_accumulation_steps 30 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 10 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --output_dir saves/Qwen1.5-7B-chat/lora/mrking_0330 \\\n",
        "    --bf16 True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_target q_proj,v_proj \\\n",
        "    --plot_loss True \\\n",
        "    --overwrite_output_dir"
      ],
      "metadata": {
        "id": "47ZSLQ2xLhQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A100 --40G\n",
        "\n",
        "\n",
        "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen1.5-7B \\\n",
        "    --finetuning_type lora \\\n",
        "    --template default \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset train_data_for_Qwen_hf \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 10.0 \\\n",
        "    --max_samples 100000 \\\n",
        "    --per_device_train_batch_size 20 \\\n",
        "    --gradient_accumulation_steps 20 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 500 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --output_dir saves/Qwen1.5-7B/lora/mrking_0326 \\\n",
        "    --bf16 True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16.0 \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_target q_proj,v_proj \\\n",
        "    --plot_loss True"
      ],
      "metadata": {
        "id": "u66YQ5VUaSyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen-7B-chat \\\n",
        "    --dataset train_data_for_qwen_hf \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora \\\n",
        "    --output_dir saves/Qwen-7B-chat/lora/mrking_0330 \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 24 \\\n",
        "    --gradient_accumulation_steps 24 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 10 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 10.0 \\\n",
        "    --plot_loss True\\\n",
        "    --bf16 True"
      ],
      "metadata": {
        "id": "gFrfdUZBnVz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelscope"
      ],
      "metadata": {
        "id": "o1EWXcN7hOy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from modelscope.hub.api import HubApi\n",
        "\n",
        "YOUR_ACCESS_TOKEN = '297bc887-49b3-4335-b95b-23ec301bad8a'\n",
        "# 请注意ModelScope平台针对SDK访问和git访问两种模式，提供两种不同的访问令牌(token)。此处请使用SDK访问令牌。\n",
        "\n",
        "\n",
        "api = HubApi()\n",
        "api.login(YOUR_ACCESS_TOKEN)\n",
        "api.push_model(\n",
        "    model_id=\"/content/LLaMA-Factory/mrking_Qwen_1.5_7B\",\n",
        "    model_dir=\"/content/LLaMA-Factory/mrking_Qwen_1.5_7B\" # 本地模型目录，要求目录中必须包含configuration.json\n",
        ")"
      ],
      "metadata": {
        "id": "Ii22BblDeuoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W712PVVEr5uN",
        "outputId": "151a37b7-16b1-490c-9d34-ad31a79ca14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      },
      "source": [
        "## Fine-tune model via Command Line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS0Qk5OR0i4Q"
      },
      "outputs": [],
      "source": [
        "from llmtuner import run_exp\n",
        "run_exp(dict(\n",
        "  stage=\"sft\",\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "  dataset=\"identity,alpaca_gpt4_en,alpaca_gpt4_zh\",\n",
        "  template=\"qwen\",\n",
        "  finetuning_type=\"lora\",\n",
        "  lora_target=\"all\",\n",
        "  output_dir=\"test_identity\",\n",
        "  per_device_train_batch_size=4,\n",
        "  gradient_accumulation_steps=4,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "  logging_steps=10,\n",
        "  save_steps=100,\n",
        "  learning_rate=1e-4,\n",
        "  num_train_epochs=5.0,\n",
        "  max_samples=500,\n",
        "  max_grad_norm=1.0,\n",
        "  fp16=True,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAtC8LzHV3bt",
        "outputId": "57896849-aded-4ba9-cbc3-b1bef7261822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "/content/drive/MyDrive/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/drive/MyDrive/LLaMA-Factory\")\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/LLaMA-Factory\")\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LukZmJQ7Y-RS",
        "outputId": "0bcd25ef-5f11-4d28-b728-480e9f984e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8evCifpWZPGz",
        "outputId": "4dfe59fc-4540-442a-8c9a-12af7293b9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/drive/MyDrive/LLaMA-Factory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m62oX0jirZw",
        "outputId": "1bdef1cc-4a55-4120-9118-57571fe757ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVNaC-xS5N40"
      },
      "source": [
        "### Infer the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "outputs": [],
      "source": [
        "from llmtuner import ChatModel\n",
        "chat_model = ChatModel(dict(\n",
        "  model_name_or_path=\"Qwen/Qwen1.5-0.5B-Chat\",\n",
        "  adapter_name_or_path=\"test_identity\",\n",
        "  finetuning_type=\"lora\",\n",
        "  template=\"qwen\",\n",
        "))\n",
        "messages = []\n",
        "while True:\n",
        "  query = input(\"\\nUser: \")\n",
        "  if query.strip() == \"exit\":\n",
        "    break\n",
        "  if query.strip() == \"clear\":\n",
        "    messages = []\n",
        "    continue\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": query})\n",
        "  print(\"Assistant: \", end=\"\", flush=True)\n",
        "  response = \"\"\n",
        "  for new_text in chat_model.stream_chat(messages):\n",
        "    print(new_text, end=\"\", flush=True)\n",
        "    response += new_text\n",
        "  print()\n",
        "  messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flmc2i3Z7Bl7"
      },
      "source": [
        "### Merge LoRA weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0fVaJsj7GC-"
      },
      "outputs": [],
      "source": [
        "from llmtuner import export_model\n",
        "export_model(dict(\n",
        "  model_name_or_path=\"Qwen/Qwen1.5-7B\",\n",
        "  adapter_name_or_path=\"test_identity\",\n",
        "  finetuning_type=\"lora\",\n",
        "  template=\"qwen\",\n",
        "  export_dir=\"test_exported\",\n",
        "  export_hub_model_id=\"jtjt520j/Qwen_1.5_7b_text_to_sql_mrking\",\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}